{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f43355-e0a2-4819-bdf2-4b74ea6f7335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料数在 3000 条以下的类别数： 736\n",
      "语料中类别对应语料数在 3000 条以下的语料总数为: 146003\n"
     ]
    }
   ],
   "source": [
    "# 对小语料类别语料数在300以下的对应语料进行统计\n",
    "import pandas as pd\n",
    "\n",
    "# 读取类别统计文件\n",
    "# category_counts_file = 'category_counts_all_new.csv'\n",
    "category_counts_file = 'res_files_FIX/FIX_deduplicated_mangoNews_category_counts_all.csv'\n",
    "category_counts_df = pd.read_csv(category_counts_file)\n",
    "\n",
    "# 过滤出类别语料数在 300 条以下的类别\n",
    "filtered_categories = category_counts_df[category_counts_df['num'] <= 3000]['category1'].tolist()\n",
    "print(\"语料数在 3000 条以下的类别数：\",len(filtered_categories))\n",
    "\n",
    "# 读取原始语料文件，并过滤出属于上述类别的语料\n",
    "# corpus_file = 'your_corpus.csv'  # 替换为你的原始语料文件路径\n",
    "# file_path = './deduplicated_mangoNews.csv'          # 去重后完整数据集（12.9g)\n",
    "# file_path = 'deduplicated_mangoNews_Example_10000.csv'          # 去重后10000行数据集（190m)\n",
    "file_path = './datasets_FIX/FIX_deduplicated_mangoNews.csv'          # 去重后完整数据集（12.9g)\n",
    "\n",
    "# file_path = 'deduplicated_mangoNews_Example_100000.csv'          # 去重后100000行数据集（2.1g)\n",
    "\n",
    "corpus_df = pd.read_csv(file_path,low_memory=False,lineterminator=\"\\n\")\n",
    "\n",
    "# 统计属于上述类别的语料总数\n",
    "filtered_corpus_count = corpus_df[corpus_df['category1'].isin(filtered_categories)].shape[0]\n",
    "\n",
    "# 输出结果\n",
    "print(f\"语料中类别对应语料数在 3000 条以下的语料总数为: {filtered_corpus_count}\")\n",
    "\n",
    "# 10000：\n",
    "# 语料数在 300 条以下的类别数： 620\n",
    "# 语料中类别对应语料数在 300 条以下的语料总数为: 2344\n",
    "# 100000:\n",
    "# 语料数在 300 条以下的类别数： 620\n",
    "# 语料中类别对应语料数在 300 条以下的语料总数为: 18863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71f35583-ba73-4308-b568-c2a8c0ac7dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料数在 3000 条以下的类别数： 736\n",
      "filtered_chunk: 66021\n",
      "66021\n",
      "filtered_chunk: 1354\n",
      "67375\n",
      "filtered_chunk: 4736\n",
      "72111\n",
      "filtered_chunk: 13554\n",
      "85665\n",
      "filtered_chunk: 11030\n",
      "96695\n",
      "filtered_chunk: 1827\n",
      "98522\n",
      "filtered_chunk: 10897\n",
      "109419\n",
      "filtered_chunk: 8200\n",
      "117619\n",
      "filtered_chunk: 5260\n",
      "122879\n",
      "filtered_chunk: 5228\n",
      "128107\n",
      "filtered_chunk: 5156\n",
      "133263\n",
      "filtered_chunk: 6497\n",
      "139760\n",
      "filtered_chunk: 2778\n",
      "142538\n",
      "filtered_chunk: 1620\n",
      "144158\n",
      "语料中类别对应语料数在 3000 条以下的语料总数为: 144158\n"
     ]
    }
   ],
   "source": [
    "# 对完整语料类别语料数在300以下的对应语料进行统计（分块）\n",
    "import pandas as pd\n",
    "\n",
    "# 读取类别统计文件\n",
    "category_counts_file = 'category_counts_all_new.csv'\n",
    "category_counts_df = pd.read_csv(category_counts_file)\n",
    "\n",
    "# # 过滤出类别语料数在 300 条以下的类别\n",
    "# filtered_categories = category_counts_df[category_counts_df['num'] <= 300]['category1'].tolist()\n",
    "# 过滤出类别语料数在 500 条以下的类别\n",
    "filtered_categories = category_counts_df[category_counts_df['num'] <= 500]['category1'].tolist()\n",
    "# 过滤出类别语料数在 1000 条以下的类别\n",
    "filtered_categories = category_counts_df[category_counts_df['num'] <= 1000]['category1'].tolist()\n",
    "# 过滤出类别语料数在 3000 条以下的类别\n",
    "filtered_categories = category_counts_df[category_counts_df['num'] <= 3000]['category1'].tolist()\n",
    "# print(\"语料数在 300 条以下的类别数：\",len(filtered_categories))\n",
    "# print(\"语料数在 500 条以下的类别数：\",len(filtered_categories))\n",
    "# print(\"语料数在 1000 条以下的类别数：\",len(filtered_categories))\n",
    "print(\"语料数在 3000 条以下的类别数：\",len(filtered_categories))\n",
    "    \n",
    "# 读取原始语料文件，并过滤出属于上述类别的语料\n",
    "# corpus_file = 'your_corpus.csv'  # 替换为你的原始语料文件路径\n",
    "file_path = './deduplicated_mangoNews.csv'          # 去重后完整数据集（12.9g)\n",
    "# file_path = 'deduplicated_mangoNews_Example_10000.csv'          # 去重后10000行数据集（190m)\n",
    "# file_path = 'deduplicated_mangoNews_Example_100000.csv'          # 去重后100000行数据集（2.1g)\n",
    "\n",
    "\n",
    "chunk_size = 100000  # 设置分块大小\n",
    "filtered_corpus_count = 0\n",
    "\n",
    "# 分块读取原始语料文件\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False,lineterminator=\"\\n\"):\n",
    "    filtered_chunk = chunk[chunk['category1'].isin(filtered_categories)]\n",
    "    print(\"filtered_chunk:\",filtered_chunk.shape[0])\n",
    "    filtered_corpus_count += filtered_chunk.shape[0]\n",
    "    print(filtered_corpus_count)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"语料中类别对应语料数在 3000 条以下的语料总数为: {filtered_corpus_count}\")\n",
    "\n",
    "# 类别总数：794\n",
    "# 类别语料数300以下：\n",
    "# 语料数在 300 条以下的类别数： 620\n",
    "# 语料中类别对应语料数在 300 条以下的语料总数为: 26843\n",
    "# 类别语料数500以下：\n",
    "# 语料数在 500 条以下的类别数： 649\n",
    "# 语料中类别对应语料数在 500 条以下的语料总数为: 38435\n",
    "# 语料数在 1000 条以下的类别数： 692\n",
    "# 语料中类别对应语料数在 1000 条以下的语料总数为: 68299\n",
    "\n",
    "# 语料数在 3000 条以下的类别数： 736  剩余类别数：794-736 = 58\n",
    "# 语料中类别对应语料数在 3000 条以下的语料总数为: 144158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e648c0cb-d39e-4b07-b4ee-f3bb784cb6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料数在 3000 条以上的类别数： 58\n",
      "large_corpus_data: 1514835\n",
      "1514835\n",
      "已将类别对应语料数在 3000 条以上的语料写入到新的 CSV 文件中。\n"
     ]
    }
   ],
   "source": [
    "# 将类别对应语料数>3000的语料写入到新文件：deduplicated_mangoNews_Nums3000p.csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 读取类别统计文件\n",
    "# category_counts_file = 'category_counts_all_new.csv'\n",
    "category_counts_file = 'res_files_FIX/FIX_deduplicated_mangoNews_category_counts_all.csv'\n",
    "category_counts_df = pd.read_csv(category_counts_file)\n",
    "\n",
    "# 获取类别语料数在 3000 条以上的类别\n",
    "large_categories = category_counts_df[category_counts_df['num'] > 3000]['category1']\n",
    "print(\"语料数在 3000 条以上的类别数：\",len(large_categories))\n",
    "\n",
    "# 读取原始语料文件，并分块处理\n",
    "file_path = './datasets_FIX/FIX_deduplicated_mangoNews.csv'          # 去重后完整数据集（12.9g)\n",
    "output_file = './datasets_FIX/FIX_deduplicated_mangoNews_Nums3000p.csv'\n",
    "\n",
    "# chunk_size = 100000  # 设置分块大小\n",
    "large_corpus_count = 0\n",
    "\n",
    "data = pd.read_csv(file_path, low_memory=False,lineterminator=\"\\n\")\n",
    "large_corpus_data = data[data['category1'].isin(large_categories)]\n",
    "print(\"large_corpus_data:\",large_corpus_data.shape[0])\n",
    "large_corpus_count = large_corpus_data.shape[0]\n",
    "print(large_corpus_count)\n",
    "\n",
    "\n",
    "large_corpus_data.to_csv(output_file, index=False)\n",
    "\n",
    "# # 分块读取原始语料文件，并筛选出类别对应语料数在 3000 条以上的语料\n",
    "# for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False,lineterminator=\"\\n\"):\n",
    "#     large_corpus_chunk = chunk[chunk['category1'].isin(large_categories)]\n",
    "    \n",
    "#     print(\"large_corpus_chunk:\",large_corpus_chunk.shape[0])\n",
    "#     large_corpus_count += large_corpus_chunk.shape[0]\n",
    "#     print(large_corpus_count)\n",
    "    \n",
    "#     # 追加写入到新的 CSV 文件中\n",
    "#     large_corpus_chunk.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))\n",
    "\n",
    "    \n",
    "print(\"已将类别对应语料数在 3000 条以上的语料写入到新的 CSV 文件中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1b3730-f0b6-469d-afa4-8f664dc16bba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料数在 0条以上的类别数： 794\n",
      "large_corpus_data: 1660838\n",
      "1660838\n",
      "语料数在 500条以上的类别数： 145\n",
      "large_corpus_data: 1622061\n",
      "1622061\n",
      "语料数在 1000条以上的类别数： 104\n",
      "large_corpus_data: 1593836\n",
      "1593836\n",
      "语料数在 1500条以上的类别数： 82\n",
      "large_corpus_data: 1565624\n",
      "1565624\n",
      "语料数在 2000条以上的类别数： 70\n",
      "large_corpus_data: 1545262\n",
      "1545262\n",
      "语料数在 3000条以上的类别数： 58\n",
      "large_corpus_data: 1514835\n",
      "1514835\n",
      "语料数在 3500条以上的类别数： 53\n",
      "large_corpus_data: 1498765\n",
      "1498765\n",
      "FINISH\n"
     ]
    }
   ],
   "source": [
    "# 将类别对应语料数>3000的语料写入到新文件：deduplicated_mangoNews_Nums3000p.csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 读取类别统计文件\n",
    "# category_counts_file = 'category_counts_all_new.csv'\n",
    "ls = [0,500,1000,1500,2000,3000,3500]\n",
    "for i in ls:\n",
    "    \n",
    "    category_counts_file = 'res_files_FIX/FIX_deduplicated_mangoNews_category_counts_all.csv'\n",
    "    category_counts_df = pd.read_csv(category_counts_file)\n",
    "\n",
    "    # 获取类别语料数在 3000 条以上的类别\n",
    "    # large_categories = category_counts_df[category_counts_df['num'] > 3000]['category1']\n",
    "    large_categories = category_counts_df[category_counts_df['num'] > i]['category1']\n",
    "\n",
    "    stri = \"语料数在 \"+str(i)+ \"条以上的类别数：\"\n",
    "    print(stri,len(large_categories))\n",
    "\n",
    "    # 读取原始语料文件，并分块处理\n",
    "    file_path = './datasets_FIX/FIX_deduplicated_mangoNews.csv'          # 去重后完整数据集（12.9g)\n",
    "    output_file = './datasets_FIX/FIX_deduplicated_mangoNews_Nums3000p.csv'\n",
    "\n",
    "    # chunk_size = 100000  # 设置分块大小\n",
    "    large_corpus_count = 0\n",
    "\n",
    "    data = pd.read_csv(file_path, low_memory=False,lineterminator=\"\\n\")\n",
    "    # print(len(data))\n",
    "    large_corpus_data = data[data['category1'].isin(large_categories)]\n",
    "    print(\"large_corpus_data:\",large_corpus_data.shape[0])\n",
    "    large_corpus_count = large_corpus_data.shape[0]\n",
    "    print(large_corpus_count)\n",
    "\n",
    "\n",
    "# large_corpus_data.to_csv(output_file, index=False)\n",
    "\n",
    "# # 分块读取原始语料文件，并筛选出类别对应语料数在 3000 条以上的语料\n",
    "# for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False,lineterminator=\"\\n\"):\n",
    "#     large_corpus_chunk = chunk[chunk['category1'].isin(large_categories)]\n",
    "    \n",
    "#     print(\"large_corpus_chunk:\",large_corpus_chunk.shape[0])\n",
    "#     large_corpus_count += large_corpus_chunk.shape[0]\n",
    "#     print(large_corpus_count)\n",
    "    \n",
    "#     # 追加写入到新的 CSV 文件中\n",
    "#     large_corpus_chunk.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))\n",
    "\n",
    "print(\"FINISH\")\n",
    "# print(\"已将类别对应语料数在 3000 条以上的语料写入到新的 CSV 文件中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ccc3bdd-8db8-484d-8d62-4c3735cea8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料数在 2500条以上的类别数： 65\n",
      "large_corpus_data: 1534146\n",
      "1534146\n",
      "FINISH\n"
     ]
    }
   ],
   "source": [
    "# 将类别对应语料数>3000的语料写入到新文件：deduplicated_mangoNews_Nums3000p.csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 读取类别统计文件\n",
    "# category_counts_file = 'category_counts_all_new.csv'\n",
    "ls = [2500]\n",
    "for i in ls:\n",
    "    \n",
    "    category_counts_file = 'res_files_FIX/FIX_deduplicated_mangoNews_category_counts_all.csv'\n",
    "    category_counts_df = pd.read_csv(category_counts_file)\n",
    "\n",
    "    # 获取类别语料数在 3000 条以上的类别\n",
    "    # large_categories = category_counts_df[category_counts_df['num'] > 3000]['category1']\n",
    "    large_categories = category_counts_df[category_counts_df['num'] > i]['category1']\n",
    "\n",
    "    stri = \"语料数在 \"+str(i)+ \"条以上的类别数：\"\n",
    "    print(stri,len(large_categories))\n",
    "\n",
    "    # 读取原始语料文件，并分块处理\n",
    "    file_path = './datasets_FIX/FIX_deduplicated_mangoNews.csv'          # 去重后完整数据集（12.9g)\n",
    "    output_file = './datasets_FIX/FIX_deduplicated_mangoNews_Nums3000p.csv'\n",
    "\n",
    "    # chunk_size = 100000  # 设置分块大小\n",
    "    large_corpus_count = 0\n",
    "\n",
    "    data = pd.read_csv(file_path, low_memory=False,lineterminator=\"\\n\")\n",
    "    # print(len(data))\n",
    "    large_corpus_data = data[data['category1'].isin(large_categories)]\n",
    "    print(\"large_corpus_data:\",large_corpus_data.shape[0])\n",
    "    large_corpus_count = large_corpus_data.shape[0]\n",
    "    print(large_corpus_count)\n",
    "\n",
    "\n",
    "# large_corpus_data.to_csv(output_file, index=False)\n",
    "\n",
    "# # 分块读取原始语料文件，并筛选出类别对应语料数在 3000 条以上的语料\n",
    "# for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False,lineterminator=\"\\n\"):\n",
    "#     large_corpus_chunk = chunk[chunk['category1'].isin(large_categories)]\n",
    "    \n",
    "#     print(\"large_corpus_chunk:\",large_corpus_chunk.shape[0])\n",
    "#     large_corpus_count += large_corpus_chunk.shape[0]\n",
    "#     print(large_corpus_count)\n",
    "    \n",
    "#     # 追加写入到新的 CSV 文件中\n",
    "#     large_corpus_chunk.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))\n",
    "\n",
    "print(\"FINISH\")\n",
    "# print(\"已将类别对应语料数在 3000 条以上的语料写入到新的 CSV 文件中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5fcab9-6483-4ec7-bef0-061f75032695",
   "metadata": {},
   "outputs": [],
   "source": [
    "语料数在 0条以上的类别数： 794\n",
    "large_corpus_data: 1660838\n",
    "1660838\n",
    "语料数在 500条以上的类别数： 145\n",
    "large_corpus_data: 1622061\n",
    "1622061\n",
    "语料数在 1000条以上的类别数： 104\n",
    "large_corpus_data: 1593836\n",
    "1593836\n",
    "语料数在 1500条以上的类别数： 82\n",
    "large_corpus_data: 1565624\n",
    "1565624\n",
    "语料数在 2000条以上的类别数： 70\n",
    "large_corpus_data: 1545262\n",
    "1545262\n",
    "语料数在 3000条以上的类别数： 58\n",
    "large_corpus_data: 1514835\n",
    "1514835\n",
    "语料数在 3500条以上的类别数： 53\n",
    "large_corpus_data: 1498765\n",
    "1498765\n",
    "FINISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8779f2a-eaa2-4af8-b9f3-64339c05a4b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1661972\n"
     ]
    }
   ],
   "source": [
    "# 读取原始语料文件，并分块处理\n",
    "file_path = './datasets_FIX/FIX_deduplicated_mangoNews.csv'          # 去重后完整数据集（12.9g)\n",
    "output_file = './datasets_FIX/FIX_deduplicated_mangoNews_Nums3000p.csv'\n",
    "\n",
    "# chunk_size = 100000  # 设置分块大小\n",
    "large_corpus_count = 0\n",
    "\n",
    "data = pd.read_csv(file_path, low_memory=False,lineterminator=\"\\n\")\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8332b0ba-d4e5-4dee-afd4-87d8741acb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(794-794)\n",
    "print(794-145)\n",
    "print(794-104)\n",
    "print(794-82)\n",
    "print(794-70)\n",
    "print(794-65)\n",
    "print(794-58)\n",
    "print(794-53)\n",
    "\n",
    "\n",
    "print(1661972-1660838)\n",
    "print(1661972-1622061)\n",
    "print(1661972-1593836)\n",
    "print(1661972-1565624)\n",
    "print(1661972-1545262)\n",
    "print(1661972-1514835)\n",
    "print(1661972-1498765)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-default",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
