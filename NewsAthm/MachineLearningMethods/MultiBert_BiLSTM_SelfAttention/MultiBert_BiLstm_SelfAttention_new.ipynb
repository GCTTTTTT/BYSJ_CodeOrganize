{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5977a74d-4135-4b75-b98b-6cefabd977cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7b1913-54e5-488c-9c8a-4f91609d2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "# 定义Self-Attention层\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        energy = self.projection(encoder_outputs)\n",
    "        weights = torch.softmax(energy.squeeze(-1), dim=1)\n",
    "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
    "        return outputs\n",
    "\n",
    "# 定义模型\n",
    "class NewsClassifier(nn.Module):\n",
    "    # hidden_size = 128\n",
    "    def __init__(self, num_classes, hidden_size=768, num_layers=2, bidirectional=True):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('../../bert-base-multilingual-cased')\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "                            bidirectional=bidirectional, batch_first=True)\n",
    "        \n",
    "        self.attention = SelfAttention(hidden_size * (2 if bidirectional else 1))\n",
    "        self.fc = nn.Linear(hidden_size * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        lstm_outputs, _ = self.lstm(last_hidden_state)\n",
    "        attention_outputs = self.attention(lstm_outputs)\n",
    "        logits = self.fc(attention_outputs)\n",
    "        return logits\n",
    "\n",
    "# 定义训练函数\n",
    "def train_model(model, dataloader, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch + 1}', leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        # progress_bar.set_postfix({'Loss': avg_loss:.4f})\n",
    "        # progress_bar.set_postfix({'Loss': avg_loss:.4f})\n",
    "        progress_bar.set_postfix({'Loss': \"{:.4f}\".format(avg_loss) })\n",
    "        \n",
    "\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "# 定义评估函数\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            batch_predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return classification_report(true_labels, predictions, digits=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbbacbb-df53-4279-84f2-0257a7f6930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载数据\n",
    "file_path = '../../datasets_FIX2/FIX2_deduplicated_mangoNews_Nums3000p_CategoryMerge_new_undersampled_Example.csv'\n",
    "# file_path = '../datasets_FIX2/FIX2_deduplicated_mangoNews_Nums3000p_CategoryMerge_new_undersampled.csv'\n",
    "\n",
    "data = pd.read_csv(file_path,low_memory=False,lineterminator=\"\\n\")\n",
    "\n",
    "texts = data['body'].tolist()\n",
    "labels = data['category1'].tolist()\n",
    "\n",
    "# 对标签进行编码\n",
    "unique_labels = list(set(labels))\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "labels = [label_to_id[label] for label in labels]\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# 加载BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('../../bert-base-multilingual-cased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22573f24-5583-4db9-82b1-60184e6ccaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Train Loss: 1.9951\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5926    0.5517    0.5714        29\n",
      "           1     0.8400    0.8400    0.8400        25\n",
      "           2     0.6667    0.9630    0.7879        27\n",
      "           3     0.5238    0.6471    0.5789        17\n",
      "           4     0.9167    0.9565    0.9362        23\n",
      "           5     1.0000    0.9048    0.9500        21\n",
      "           6     0.7143    0.6250    0.6667        16\n",
      "           7     0.7391    0.7083    0.7234        24\n",
      "           8     1.0000    0.4444    0.6154        18\n",
      "\n",
      "    accuracy                         0.7500       200\n",
      "   macro avg     0.7770    0.7379    0.7411       200\n",
      "weighted avg     0.7717    0.7500    0.7464       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "Train Loss: 1.4363\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7037    0.6552    0.6786        29\n",
      "           1     0.8519    0.9200    0.8846        25\n",
      "           2     0.7419    0.8519    0.7931        27\n",
      "           3     0.6316    0.7059    0.6667        17\n",
      "           4     0.9167    0.9565    0.9362        23\n",
      "           5     0.9091    0.9524    0.9302        21\n",
      "           6     0.6000    0.7500    0.6667        16\n",
      "           7     0.8947    0.7083    0.7907        24\n",
      "           8     1.0000    0.6111    0.7586        18\n",
      "\n",
      "    accuracy                         0.7950       200\n",
      "   macro avg     0.8055    0.7901    0.7895       200\n",
      "weighted avg     0.8086    0.7950    0.7945       200\n",
      "\n",
      "Fold 1 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7037    0.6552    0.6786        29\n",
      "           1     0.8519    0.9200    0.8846        25\n",
      "           2     0.7419    0.8519    0.7931        27\n",
      "           3     0.6316    0.7059    0.6667        17\n",
      "           4     0.9167    0.9565    0.9362        23\n",
      "           5     0.9091    0.9524    0.9302        21\n",
      "           6     0.6000    0.7500    0.6667        16\n",
      "           7     0.8947    0.7083    0.7907        24\n",
      "           8     1.0000    0.6111    0.7586        18\n",
      "\n",
      "    accuracy                         0.7950       200\n",
      "   macro avg     0.8055    0.7901    0.7895       200\n",
      "weighted avg     0.8086    0.7950    0.7945       200\n",
      "\n",
      "\n",
      "Fold 2\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Train Loss: 1.9848\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    0.5000    0.5882        20\n",
      "           1     0.6667    0.7826    0.7200        23\n",
      "           2     0.5526    0.9545    0.7000        22\n",
      "           3     0.6923    0.6429    0.6667        28\n",
      "           4     0.9615    1.0000    0.9804        25\n",
      "           5     0.9444    0.7727    0.8500        22\n",
      "           6     0.9286    0.5652    0.7027        23\n",
      "           7     0.6875    0.7857    0.7333        14\n",
      "           8     0.7619    0.6957    0.7273        23\n",
      "\n",
      "    accuracy                         0.7450       200\n",
      "   macro avg     0.7678    0.7444    0.7410       200\n",
      "weighted avg     0.7724    0.7450    0.7438       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "Train Loss: 1.4170\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7857    0.5500    0.6471        20\n",
      "           1     0.6538    0.7391    0.6939        23\n",
      "           2     0.6774    0.9545    0.7925        22\n",
      "           3     0.6897    0.7143    0.7018        28\n",
      "           4     1.0000    0.9600    0.9796        25\n",
      "           5     0.9524    0.9091    0.9302        22\n",
      "           6     0.9412    0.6957    0.8000        23\n",
      "           7     0.7500    0.8571    0.8000        14\n",
      "           8     0.7273    0.6957    0.7111        23\n",
      "\n",
      "    accuracy                         0.7850       200\n",
      "   macro avg     0.7975    0.7862    0.7840       200\n",
      "weighted avg     0.7990    0.7850    0.7845       200\n",
      "\n",
      "Fold 2 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7857    0.5500    0.6471        20\n",
      "           1     0.6538    0.7391    0.6939        23\n",
      "           2     0.6774    0.9545    0.7925        22\n",
      "           3     0.6897    0.7143    0.7018        28\n",
      "           4     1.0000    0.9600    0.9796        25\n",
      "           5     0.9524    0.9091    0.9302        22\n",
      "           6     0.9412    0.6957    0.8000        23\n",
      "           7     0.7500    0.8571    0.8000        14\n",
      "           8     0.7273    0.6957    0.7111        23\n",
      "\n",
      "    accuracy                         0.7850       200\n",
      "   macro avg     0.7975    0.7862    0.7840       200\n",
      "weighted avg     0.7990    0.7850    0.7845       200\n",
      "\n",
      "\n",
      "Fold 3\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Train Loss: 1.9999\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.4000    0.5000        20\n",
      "           1     0.8333    0.9091    0.8696        22\n",
      "           2     0.9048    0.8636    0.8837        22\n",
      "           3     0.8095    0.8095    0.8095        21\n",
      "           4     0.8636    1.0000    0.9268        19\n",
      "           5     0.8947    0.8500    0.8718        20\n",
      "           6     0.8966    0.8125    0.8525        32\n",
      "           7     0.4706    0.8000    0.5926        20\n",
      "           8     0.9444    0.7083    0.8095        24\n",
      "\n",
      "    accuracy                         0.7950       200\n",
      "   macro avg     0.8094    0.7948    0.7907       200\n",
      "weighted avg     0.8182    0.7950    0.7959       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "Train Loss: 1.4729\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6500    0.6500    0.6500        20\n",
      "           1     0.8077    0.9545    0.8750        22\n",
      "           2     0.7097    1.0000    0.8302        22\n",
      "           3     0.8571    0.8571    0.8571        21\n",
      "           4     0.9500    1.0000    0.9744        19\n",
      "           5     0.9474    0.9000    0.9231        20\n",
      "           6     0.8889    0.7500    0.8136        32\n",
      "           7     0.8750    0.7000    0.7778        20\n",
      "           8     0.9000    0.7500    0.8182        24\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.8429    0.8402    0.8355       200\n",
      "weighted avg     0.8446    0.8350    0.8336       200\n",
      "\n",
      "Fold 3 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6500    0.6500    0.6500        20\n",
      "           1     0.8077    0.9545    0.8750        22\n",
      "           2     0.7097    1.0000    0.8302        22\n",
      "           3     0.8571    0.8571    0.8571        21\n",
      "           4     0.9500    1.0000    0.9744        19\n",
      "           5     0.9474    0.9000    0.9231        20\n",
      "           6     0.8889    0.7500    0.8136        32\n",
      "           7     0.8750    0.7000    0.7778        20\n",
      "           8     0.9000    0.7500    0.8182        24\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.8429    0.8402    0.8355       200\n",
      "weighted avg     0.8446    0.8350    0.8336       200\n",
      "\n",
      "\n",
      "Fold 4\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Train Loss: 1.9442\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7391    0.6800    0.7083        25\n",
      "           1     0.7647    0.9286    0.8387        28\n",
      "           2     0.7353    0.9259    0.8197        27\n",
      "           3     0.5946    1.0000    0.7458        22\n",
      "           4     1.0000    0.9062    0.9508        32\n",
      "           5     1.0000    1.0000    1.0000         7\n",
      "           6     0.8000    0.8000    0.8000        15\n",
      "           7     1.0000    0.4762    0.6452        21\n",
      "           8     0.8182    0.3913    0.5294        23\n",
      "\n",
      "    accuracy                         0.7850       200\n",
      "   macro avg     0.8280    0.7898    0.7820       200\n",
      "weighted avg     0.8182    0.7850    0.7744       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "Train Loss: 1.3900\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7917    0.7600    0.7755        25\n",
      "           1     0.8966    0.9286    0.9123        28\n",
      "           2     0.8125    0.9630    0.8814        27\n",
      "           3     0.6667    1.0000    0.8000        22\n",
      "           4     0.9688    0.9688    0.9688        32\n",
      "           5     0.8750    1.0000    0.9333         7\n",
      "           6     0.7647    0.8667    0.8125        15\n",
      "           7     0.9231    0.5714    0.7059        21\n",
      "           8     0.8333    0.4348    0.5714        23\n",
      "\n",
      "    accuracy                         0.8300       200\n",
      "   macro avg     0.8369    0.8326    0.8179       200\n",
      "weighted avg     0.8432    0.8300    0.8201       200\n",
      "\n",
      "Fold 4 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7917    0.7600    0.7755        25\n",
      "           1     0.8966    0.9286    0.9123        28\n",
      "           2     0.8125    0.9630    0.8814        27\n",
      "           3     0.6667    1.0000    0.8000        22\n",
      "           4     0.9688    0.9688    0.9688        32\n",
      "           5     0.8750    1.0000    0.9333         7\n",
      "           6     0.7647    0.8667    0.8125        15\n",
      "           7     0.9231    0.5714    0.7059        21\n",
      "           8     0.8333    0.4348    0.5714        23\n",
      "\n",
      "    accuracy                         0.8300       200\n",
      "   macro avg     0.8369    0.8326    0.8179       200\n",
      "weighted avg     0.8432    0.8300    0.8201       200\n",
      "\n",
      "\n",
      "Fold 5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Train Loss: 1.9606\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6111    0.4583    0.5238        24\n",
      "           1     0.6429    0.9000    0.7500        20\n",
      "           2     0.7391    0.8095    0.7727        21\n",
      "           3     0.6970    0.8846    0.7797        26\n",
      "           4     0.8750    0.9545    0.9130        22\n",
      "           5     0.9167    0.8800    0.8980        25\n",
      "           6     0.8571    0.8182    0.8372        22\n",
      "           7     1.0000    0.6957    0.8205        23\n",
      "           8     0.7692    0.5882    0.6667        17\n",
      "\n",
      "    accuracy                         0.7800       200\n",
      "   macro avg     0.7898    0.7766    0.7735       200\n",
      "weighted avg     0.7913    0.7800    0.7761       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "Train Loss: 1.4003\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6364    0.5833    0.6087        24\n",
      "           1     0.7826    0.9000    0.8372        20\n",
      "           2     0.7500    0.8571    0.8000        21\n",
      "           3     0.8400    0.8077    0.8235        26\n",
      "           4     1.0000    0.9545    0.9767        22\n",
      "           5     0.8800    0.8800    0.8800        25\n",
      "           6     0.8636    0.8636    0.8636        22\n",
      "           7     0.7826    0.7826    0.7826        23\n",
      "           8     0.8000    0.7059    0.7500        17\n",
      "\n",
      "    accuracy                         0.8150       200\n",
      "   macro avg     0.8150    0.8150    0.8136       200\n",
      "weighted avg     0.8156    0.8150    0.8140       200\n",
      "\n",
      "Fold 5 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6364    0.5833    0.6087        24\n",
      "           1     0.7826    0.9000    0.8372        20\n",
      "           2     0.7500    0.8571    0.8000        21\n",
      "           3     0.8400    0.8077    0.8235        26\n",
      "           4     1.0000    0.9545    0.9767        22\n",
      "           5     0.8800    0.8800    0.8800        25\n",
      "           6     0.8636    0.8636    0.8636        22\n",
      "           7     0.7826    0.7826    0.7826        23\n",
      "           8     0.8000    0.7059    0.7500        17\n",
      "\n",
      "    accuracy                         0.8150       200\n",
      "   macro avg     0.8150    0.8150    0.8136       200\n",
      "weighted avg     0.8156    0.8150    0.8140       200\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 设置超参数\n",
    "max_length = 256\n",
    "batch_size = 16\n",
    "epochs = 2\n",
    "learning_rate = 2e-5\n",
    "hidden_size = 768\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "\n",
    "# 使用KFold进行交叉验证\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "best_models = []\n",
    "best_reports = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(texts, labels)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    print('-' * 30)\n",
    "\n",
    "    train_texts, val_texts = [texts[i] for i in train_index], [texts[i] for i in val_index]\n",
    "    train_labels, val_labels = [labels[i] for i in train_index], [labels[i] for i in val_index]\n",
    "\n",
    "    train_dataset = NewsDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "    val_dataset = NewsDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = NewsClassifier(num_classes, hidden_size, num_layers, bidirectional)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_model(model, train_dataloader, optimizer, scheduler, device, epoch)\n",
    "        val_report = evaluate_model(model, val_dataloader, device)\n",
    "\n",
    "        val_loss = 1 - float(val_report.split('\\n')[-3].split()[-2])  # 提取验证集损失\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        print(f'Train Loss: {train_loss:.4f}')\n",
    "        print('Validation Report:')\n",
    "        print(val_report)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # best_model = model.state_dict()\n",
    "            # best_report = val_report\n",
    "            torch.save(model.state_dict(), f'best_MultiBert_BiLSTM_SelfAttention_model_fold_{fold + 1}.pth')\n",
    "\n",
    "\n",
    "    # print(f'Fold {fold + 1} Best Validation Results:')\n",
    "    # print(best_report)\n",
    "    # print()\n",
    "\n",
    "# 在每个fold结束后,评估最佳模型在验证集上的性能\n",
    "    # best_model = LSTMClassifier(bert_model, hidden_size, num_classes)\n",
    "    best_model = NewsClassifier(num_classes, hidden_size, num_layers, bidirectional)\n",
    "\n",
    "    best_model.load_state_dict(torch.load(f'best_MultiBert_BiLSTM_SelfAttention_model_fold_{fold + 1}.pth'))\n",
    "    best_model.to(device)\n",
    "    val_report = evaluate_model(best_model, val_dataloader, device)\n",
    "    # all_reports.append(val_report)\n",
    "\n",
    "    print(f'Fold {fold + 1} Best Validation Report:')\n",
    "    print(val_report)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a12ff6-953c-48d4-adf9-c7053f4f1bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-default",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
