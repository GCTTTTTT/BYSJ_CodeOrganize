{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f574e337-b71d-46a9-9992-ca6529643198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9917715d-7a2b-47f0-8fb1-6165a9cdb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 定义模型\n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('../../bert-base-multilingual-cased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(dropout_output)\n",
    "        return logits\n",
    "\n",
    "# 定义训练函数\n",
    "def train_model(model, dataloader, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch}', leave=False)\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch: {epoch}, Training Loss: {avg_loss:.4f}')\n",
    "\n",
    "\n",
    "# 定义评估函数\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            batch_predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return classification_report(true_labels, predictions, digits=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb3a902-91bb-4bd8-bf13-ae2e50b3d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载数据\n",
    "file_path = '../../datasets_FIX2/FIX2_deduplicated_mangoNews_Nums3000p_CategoryMerge_new_undersampled_Example.csv'\n",
    "# file_path = '../datasets_FIX2/FIX2_deduplicated_mangoNews_Nums3000p_CategoryMerge_new_undersampled.csv'\n",
    "\n",
    "data = pd.read_csv(file_path,low_memory=False,lineterminator=\"\\n\")\n",
    "\n",
    "texts = data['body'].tolist()\n",
    "labels = data['category1'].tolist()\n",
    "\n",
    "# 对标签进行编码\n",
    "unique_labels = list(set(labels))\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "labels = [label_to_id[label] for label in labels]\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# 加载BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('../../bert-base-multilingual-cased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a7393b-0143-4d49-8c69-47fd00a7b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 1.9395\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8500    0.8095    0.8293        21\n",
      "           1     0.8571    0.2222    0.3529        27\n",
      "           2     0.6000    0.5294    0.5625        17\n",
      "           3     0.3889    0.2414    0.2979        29\n",
      "           4     1.0000    0.1250    0.2222        24\n",
      "           5     0.9167    0.6111    0.7333        18\n",
      "           6     0.8519    1.0000    0.9200        23\n",
      "           7     0.3871    0.9600    0.5517        25\n",
      "           8     0.3889    0.8750    0.5385        16\n",
      "\n",
      "    accuracy                         0.5700       200\n",
      "   macro avg     0.6934    0.5971    0.5565       200\n",
      "weighted avg     0.6923    0.5700    0.5362       200\n",
      "\n",
      "Saved best model for fold 1 with F1 score: 0.5362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 1.1877\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.9524    0.8696        21\n",
      "           1     0.7097    0.8148    0.7586        27\n",
      "           2     0.6250    0.5882    0.6061        17\n",
      "           3     0.7059    0.4138    0.5217        29\n",
      "           4     0.9375    0.6250    0.7500        24\n",
      "           5     0.9167    0.6111    0.7333        18\n",
      "           6     0.8846    1.0000    0.9388        23\n",
      "           7     0.8276    0.9600    0.8889        25\n",
      "           8     0.5000    0.8750    0.6364        16\n",
      "\n",
      "    accuracy                         0.7550       200\n",
      "   macro avg     0.7674    0.7600    0.7448       200\n",
      "weighted avg     0.7755    0.7550    0.7469       200\n",
      "\n",
      "Saved best model for fold 1 with F1 score: 0.7469\n",
      "\n",
      "Fold 1 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.9524    0.8696        21\n",
      "           1     0.7097    0.8148    0.7586        27\n",
      "           2     0.6250    0.5882    0.6061        17\n",
      "           3     0.7059    0.4138    0.5217        29\n",
      "           4     0.9375    0.6250    0.7500        24\n",
      "           5     0.9167    0.6111    0.7333        18\n",
      "           6     0.8846    1.0000    0.9388        23\n",
      "           7     0.8276    0.9600    0.8889        25\n",
      "           8     0.5000    0.8750    0.6364        16\n",
      "\n",
      "    accuracy                         0.7550       200\n",
      "   macro avg     0.7674    0.7600    0.7448       200\n",
      "weighted avg     0.7755    0.7550    0.7469       200\n",
      "\n",
      "\n",
      "Fold 2\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 1.9455\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9412    0.7273    0.8205        22\n",
      "           1     0.7200    0.8182    0.7660        22\n",
      "           2     0.7143    0.7143    0.7143        28\n",
      "           3     0.5882    0.5000    0.5405        20\n",
      "           4     0.8182    0.6429    0.7200        14\n",
      "           5     0.7273    0.6957    0.7111        23\n",
      "           6     0.9259    1.0000    0.9615        25\n",
      "           7     0.5946    0.9565    0.7333        23\n",
      "           8     0.8125    0.5652    0.6667        23\n",
      "\n",
      "    accuracy                         0.7450       200\n",
      "   macro avg     0.7602    0.7356    0.7371       200\n",
      "weighted avg     0.7600    0.7450    0.7419       200\n",
      "\n",
      "Saved best model for fold 2 with F1 score: 0.7419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 1.2122\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9091    0.9524        22\n",
      "           1     0.6471    1.0000    0.7857        22\n",
      "           2     0.7200    0.6429    0.6792        28\n",
      "           3     0.7333    0.5500    0.6286        20\n",
      "           4     0.7500    0.8571    0.8000        14\n",
      "           5     0.7083    0.7391    0.7234        23\n",
      "           6     1.0000    0.9600    0.9796        25\n",
      "           7     0.8095    0.7391    0.7727        23\n",
      "           8     0.7619    0.6957    0.7273        23\n",
      "\n",
      "    accuracy                         0.7850       200\n",
      "   macro avg     0.7922    0.7881    0.7832       200\n",
      "weighted avg     0.7950    0.7850    0.7833       200\n",
      "\n",
      "Saved best model for fold 2 with F1 score: 0.7833\n",
      "\n",
      "Fold 2 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9091    0.9524        22\n",
      "           1     0.6471    1.0000    0.7857        22\n",
      "           2     0.7200    0.6429    0.6792        28\n",
      "           3     0.7333    0.5500    0.6286        20\n",
      "           4     0.7500    0.8571    0.8000        14\n",
      "           5     0.7083    0.7391    0.7234        23\n",
      "           6     1.0000    0.9600    0.9796        25\n",
      "           7     0.8095    0.7391    0.7727        23\n",
      "           8     0.7619    0.6957    0.7273        23\n",
      "\n",
      "    accuracy                         0.7850       200\n",
      "   macro avg     0.7922    0.7881    0.7832       200\n",
      "weighted avg     0.7950    0.7850    0.7833       200\n",
      "\n",
      "\n",
      "Fold 3\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 2.0293\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8182    0.9000    0.8571        20\n",
      "           1     0.7308    0.8636    0.7917        22\n",
      "           2     0.7727    0.8095    0.7907        21\n",
      "           3     0.2462    0.8000    0.3765        20\n",
      "           4     1.0000    0.0500    0.0952        20\n",
      "           5     1.0000    0.0417    0.0800        24\n",
      "           6     0.9500    1.0000    0.9744        19\n",
      "           7     0.4634    0.8636    0.6032        22\n",
      "           8     1.0000    0.0625    0.1176        32\n",
      "\n",
      "    accuracy                         0.5600       200\n",
      "   macro avg     0.7757    0.5990    0.5207       200\n",
      "weighted avg     0.7892    0.5600    0.4903       200\n",
      "\n",
      "Saved best model for fold 3 with F1 score: 0.4903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 1.2958\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9500    0.9500    0.9500        20\n",
      "           1     0.7857    1.0000    0.8800        22\n",
      "           2     0.7826    0.8571    0.8182        21\n",
      "           3     0.5806    0.9000    0.7059        20\n",
      "           4     0.8182    0.4500    0.5806        20\n",
      "           5     0.9286    0.5417    0.6842        24\n",
      "           6     1.0000    1.0000    1.0000        19\n",
      "           7     0.6786    0.8636    0.7600        22\n",
      "           8     0.8462    0.6875    0.7586        32\n",
      "\n",
      "    accuracy                         0.7950       200\n",
      "   macro avg     0.8189    0.8055    0.7931       200\n",
      "weighted avg     0.8199    0.7950    0.7884       200\n",
      "\n",
      "Saved best model for fold 3 with F1 score: 0.7884\n",
      "\n",
      "Fold 3 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9500    0.9500    0.9500        20\n",
      "           1     0.7857    1.0000    0.8800        22\n",
      "           2     0.7826    0.8571    0.8182        21\n",
      "           3     0.5806    0.9000    0.7059        20\n",
      "           4     0.8182    0.4500    0.5806        20\n",
      "           5     0.9286    0.5417    0.6842        24\n",
      "           6     1.0000    1.0000    1.0000        19\n",
      "           7     0.6786    0.8636    0.7600        22\n",
      "           8     0.8462    0.6875    0.7586        32\n",
      "\n",
      "    accuracy                         0.7950       200\n",
      "   macro avg     0.8189    0.8055    0.7931       200\n",
      "weighted avg     0.8199    0.7950    0.7884       200\n",
      "\n",
      "\n",
      "Fold 4\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 2.0685\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8750    1.0000    0.9333         7\n",
      "           1     0.6098    0.9259    0.7353        27\n",
      "           2     0.7000    0.9545    0.8077        22\n",
      "           3     0.7407    0.8000    0.7692        25\n",
      "           4     0.8889    0.3810    0.5333        21\n",
      "           5     0.7500    0.3913    0.5143        23\n",
      "           6     0.9091    0.9375    0.9231        32\n",
      "           7     0.9048    0.6786    0.7755        28\n",
      "           8     0.6316    0.8000    0.7059        15\n",
      "\n",
      "    accuracy                         0.7550       200\n",
      "   macro avg     0.7789    0.7632    0.7442       200\n",
      "weighted avg     0.7816    0.7550    0.7413       200\n",
      "\n",
      "Saved best model for fold 4 with F1 score: 0.7413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 1.3840\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    1.0000    0.8235         7\n",
      "           1     0.7333    0.8148    0.7719        27\n",
      "           2     0.6364    0.9545    0.7636        22\n",
      "           3     0.7000    0.8400    0.7636        25\n",
      "           4     1.0000    0.4286    0.6000        21\n",
      "           5     0.8889    0.3478    0.5000        23\n",
      "           6     0.9394    0.9688    0.9538        32\n",
      "           7     0.8889    0.8571    0.8727        28\n",
      "           8     0.6842    0.8667    0.7647        15\n",
      "\n",
      "    accuracy                         0.7800       200\n",
      "   macro avg     0.7968    0.7865    0.7571       200\n",
      "weighted avg     0.8143    0.7800    0.7651       200\n",
      "\n",
      "Saved best model for fold 4 with F1 score: 0.7651\n",
      "\n",
      "Fold 4 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    1.0000    0.8235         7\n",
      "           1     0.7333    0.8148    0.7719        27\n",
      "           2     0.6364    0.9545    0.7636        22\n",
      "           3     0.7000    0.8400    0.7636        25\n",
      "           4     1.0000    0.4286    0.6000        21\n",
      "           5     0.8889    0.3478    0.5000        23\n",
      "           6     0.9394    0.9688    0.9538        32\n",
      "           7     0.8889    0.8571    0.8727        28\n",
      "           8     0.6842    0.8667    0.7647        15\n",
      "\n",
      "    accuracy                         0.7800       200\n",
      "   macro avg     0.7968    0.7865    0.7571       200\n",
      "weighted avg     0.8143    0.7800    0.7651       200\n",
      "\n",
      "\n",
      "Fold 5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 1.8662\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8846    0.9200    0.9020        25\n",
      "           1     0.8000    0.5714    0.6667        21\n",
      "           2     0.8462    0.8462    0.8462        26\n",
      "           3     0.7222    0.5417    0.6190        24\n",
      "           4     0.7826    0.7826    0.7826        23\n",
      "           5     0.8462    0.6471    0.7333        17\n",
      "           6     1.0000    0.9545    0.9767        22\n",
      "           7     0.6250    1.0000    0.7692        20\n",
      "           8     0.6923    0.8182    0.7500        22\n",
      "\n",
      "    accuracy                         0.7900       200\n",
      "   macro avg     0.7999    0.7868    0.7829       200\n",
      "weighted avg     0.8018    0.7900    0.7862       200\n",
      "\n",
      "Saved best model for fold 5 with F1 score: 0.7862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 1.1195\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8929    1.0000    0.9434        25\n",
      "           1     0.7308    0.9048    0.8085        21\n",
      "           2     0.8750    0.8077    0.8400        26\n",
      "           3     0.7500    0.5000    0.6000        24\n",
      "           4     0.8000    0.8696    0.8333        23\n",
      "           5     0.8462    0.6471    0.7333        17\n",
      "           6     1.0000    0.9545    0.9767        22\n",
      "           7     0.7917    0.9500    0.8636        20\n",
      "           8     0.8261    0.8636    0.8444        22\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.8347    0.8330    0.8270       200\n",
      "weighted avg     0.8360    0.8350    0.8289       200\n",
      "\n",
      "Saved best model for fold 5 with F1 score: 0.8289\n",
      "\n",
      "Fold 5 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8929    1.0000    0.9434        25\n",
      "           1     0.7308    0.9048    0.8085        21\n",
      "           2     0.8750    0.8077    0.8400        26\n",
      "           3     0.7500    0.5000    0.6000        24\n",
      "           4     0.8000    0.8696    0.8333        23\n",
      "           5     0.8462    0.6471    0.7333        17\n",
      "           6     1.0000    0.9545    0.9767        22\n",
      "           7     0.7917    0.9500    0.8636        20\n",
      "           8     0.8261    0.8636    0.8444        22\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.8347    0.8330    0.8270       200\n",
      "weighted avg     0.8360    0.8350    0.8289       200\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 设置超参数\n",
    "max_length = 256\n",
    "batch_size = 16\n",
    "epochs = 2\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# 使用KFold进行交叉验证\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(texts, labels)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    print('-' * 30)\n",
    "\n",
    "    train_texts, val_texts = [texts[i] for i in train_index], [texts[i] for i in val_index]\n",
    "    train_labels, val_labels = [labels[i] for i in train_index], [labels[i] for i in val_index]\n",
    "\n",
    "    train_dataset = NewsDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "    val_dataset = NewsDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = NewsClassifier(num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    best_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        train_model(model, train_dataloader, optimizer, scheduler, device, epoch)\n",
    "\n",
    "        print('Validation Results:')\n",
    "        report = evaluate_model(model, val_dataloader, device)\n",
    "        print(report)\n",
    "\n",
    "        # 保存最优模型\n",
    "        f1 = float(report.split('\\n')[-2].split()[-2])\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), f'best_MultiBert_BiLSTM_SelfAttention_model_fold_{fold + 1}.pth')\n",
    "            print(f'Saved best model for fold {fold + 1} with F1 score: {best_f1:.4f}')\n",
    "\n",
    "    print()\n",
    "    # 在每个fold结束后,评估最佳模型在验证集上的性能\n",
    "    best_model = NewsClassifier(num_classes)\n",
    "    best_model.load_state_dict(torch.load(f'best_MultiBert_BiLSTM_SelfAttention_model_fold_{fold + 1}.pth'))\n",
    "    best_model.to(device)\n",
    "    val_report = evaluate_model(best_model, val_dataloader, device)\n",
    "    # all_reports.append(val_report)\n",
    "\n",
    "    print(f'Fold {fold + 1} Best Validation Report:')\n",
    "    print(val_report)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85069ab0-14e3-4317-897b-879dc1da21e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-default",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
