{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "537b75d0-82ea-4cfd-9bdf-bc8d5b3dacd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# 配置日志记录器\n",
    "logging.basicConfig(filename='test_training.log', level=logging.INFO, format='%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7d66af-1586-44a3-b7d9-ddf3aa478d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Counts:\n",
      "category1\n",
      "কাজী নজরুল ইসলাম                203\n",
      "সুকুমার রায়                    145\n",
      "মাইকেল মধুসূদন দত্ত             133\n",
      "জসীমউদ্দীন                       96\n",
      "হেলাল হাফিজ                      59\n",
      "সুনীল গঙ্গোপাধ্যায়               52\n",
      "আবিদ আনোয়ার                     41\n",
      "writer                           41\n",
      "রবীন্দ্রনাথ ঠাকুর                33\n",
      "সত্যেন্দ্রনাথ দত্ত               21\n",
      "সুফিয়া কামাল                     20\n",
      "Onubad - Copy                    20\n",
      "Front Page-Detective Fiction     18\n",
      "কামিনী রায়                       17\n",
      "Front Page-Short Stories         16\n",
      "amader-kotha                     14\n",
      "Front Page-Poetry                14\n",
      "Front Page-Science Fiction       13\n",
      "Front Page-Tales of Unease       10\n",
      "ঈশ্বরচন্দ্র গুপ্ত                 8\n",
      "নারী ও সমাজ                       4\n",
      "Front Page-Autobiography          3\n",
      "বিজ্ঞান ও প্রযুক্তি               3\n",
      "বই-টই                             3\n",
      "স্মরণে                            3\n",
      "ব্যক্তিগত গদ্য                    2\n",
      "শিল্প- সংস্কৃতি                   2\n",
      "রম্য রচনা                         1\n",
      "ভ্রমণ                             1\n",
      "সাহিত্য                           1\n",
      "ক্রীড়াজগৎ                         1\n",
      "দর্শন- রাজনীতি- ইতিহাস            1\n",
      "সম্পাদকীয়                         1\n",
      "Name: count, dtype: int64\n",
      "{'amader-kotha': 0, 'writer': 1, 'সম্পাদকীয়': 2, 'বই-টই': 3, 'দর্শন- রাজনীতি- ইতিহাস': 4, 'ক্রীড়াজগৎ': 5, 'রম্য রচনা': 6, 'ভ্রমণ': 7, 'ব্যক্তিগত গদ্য': 8, 'স্মরণে': 9, 'নারী ও সমাজ': 10, 'শিল্প- সংস্কৃতি': 11, 'বিজ্ঞান ও প্রযুক্তি': 12, 'সাহিত্য': 13, 'ঈশ্বরচন্দ্র গুপ্ত': 14, 'সুকুমার রায়': 15, 'জসীমউদ্দীন': 16, 'কাজী নজরুল ইসলাম': 17, 'কামিনী রায়': 18, 'সত্যেন্দ্রনাথ দত্ত': 19, 'Front Page-Autobiography': 20, 'Front Page-Tales of Unease': 21, 'Front Page-Short Stories': 22, 'Front Page-Science Fiction': 23, 'Front Page-Detective Fiction': 24, 'Front Page-Poetry': 25, 'Onubad - Copy': 26, 'হেলাল হাফিজ': 27, 'মাইকেল মধুসূদন দত্ত': 28, 'সুফিয়া কামাল': 29, 'সুনীল গঙ্গোপাধ্যায়': 30, 'আবিদ আনোয়ার': 31, 'রবীন্দ্রনাথ ঠাকুর': 32}\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "# 读取CSV文件\n",
    "file_path = 'mangoNews_Example.csv'  # 测试版数据集（规模小5m）\n",
    "# file_path = 'mangoNews.csv'          # 完整版数据集（13g） 需要分块读\n",
    "# file_path = 'mangoNews_Example_100000.csv'          # 100000行数据集（2.2g）\n",
    "# file_path = 'mangoNews_Example_10000.csv'          # 10000行数据集（190m)\n",
    "# file_path = './deduplicated_mangoNews_Nums3000p_CategoryMerge.csv'          # 去重后保留类别对应语料数在3000+并合并同义类别的数据集（9.9g) 需要分块读\n",
    "# file_path = './deduplicated_mangoNews_Nums3000p_CategoryMerge_100000_1.csv'          # 去重后保留类别对应语料数在3000+并合并同义类别的数据集（576m) \n",
    "# file_path = './deduplicated_mangoNews_Nums3000p_CategoryMerge_990000.csv'          # 去重后保留类别对应语料数在3000+并合并同义类别的数据集（8.1g) \n",
    "# file_path = './deduplicated_mangoNews_Nums3000p_CategoryMerge_new_undersampled.csv'          # 去重后保留类别对应语料数在3000+并合并同义类别(新）并随机欠采样数据平衡的数据集（12095*12条 1.2g) \n",
    "\n",
    "\n",
    "data = pd.read_csv(file_path,low_memory=False,lineterminator=\"\\n\")\n",
    "\n",
    "# Select relevant columns\n",
    "data = data[['body', 'category1']]\n",
    "\n",
    "# 统计 'category1' 列中每种类别的个数\n",
    "category_counts = data['category1'].value_counts()\n",
    "\n",
    "# 设置显示选项，完整输出结果\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(\"Category Counts:\")\n",
    "print(category_counts)\n",
    "# 恢复默认显示选项\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "# 将类别列转换为整数标签  注意是data['category1']\n",
    "label_to_id = {label: idx for idx, label in enumerate(data['category1'].unique())}\n",
    "print(label_to_id)\n",
    "data['label'] = data['category1'].map(label_to_id)\n",
    "num_classes = len(label_to_id)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99239a19-093e-4382-a956-e701cf927cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "No NaN values in the 'category1' column.\n",
      "after\n",
      "No NaN values in the 'category1' column.\n",
      "['i', 'have', 'a', 'good', 'time', ',', 'thank', 'you', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./uncased_L-12_H-768_A-12 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 划分训练集和测试集\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 检验 'category1' 列是否还有 NaN 值\n",
    "# nan_check = train_data['category1'].isnull().sum()\n",
    "nan_check = train_data['category1'].isna().sum()\n",
    "\n",
    "print(\"before\")\n",
    "if nan_check > 0:\n",
    "    print(f\"There are still {nan_check} NaN values in the 'category1' column.\")\n",
    "else:\n",
    "    print(\"No NaN values in the 'category1' column.\")\n",
    "    \n",
    "# 去除包含缺失值的样本\n",
    "train_data = train_data.dropna(subset=['category1'])\n",
    "test_data = test_data.dropna(subset=['category1'])\n",
    "# todo：注意后面要提取缺失值\n",
    "\n",
    "print(\"after\")\n",
    "# 检验 'category1' 列是否还有 NaN 值\n",
    "# nan_check = train_data['category1'].isnull().sum()\n",
    "nan_check = train_data['category1'].isna().sum()\n",
    "\n",
    "if nan_check > 0:\n",
    "    print(f\"There are still {nan_check} NaN values in the 'category1' column.\")\n",
    "else:\n",
    "    print(\"No NaN values in the 'category1' column.\")\n",
    "\n",
    "Bert_path = './uncased_L-12_H-768_A-12'  # bert-base-uncased from github\n",
    "# Bert_path = './wwm_uncased_L-24_H-1024_A-16'  # bert-large-uncased(whole word masking) from github\n",
    "\n",
    "# 初始化Bert tokenizer和模型\n",
    "tokenizer = BertTokenizer.from_pretrained(Bert_path) # bert: base or large（wwm&origin） \n",
    "\n",
    "print(tokenizer.tokenize('I have a good time, thank you.')) # 测试\n",
    "\n",
    "num_classes = len(data['category1'].unique())  # num_labels 表示分类任务中唯一类别的数量\n",
    "model = BertForSequenceClassification.from_pretrained(Bert_path, num_labels=num_classes)\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94d8d18-d532-48e6-a065-701047763be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 将类别列转换为整数标签  注意是data['category1']\n",
    "# label_to_id = {label: idx for idx, label in enumerate(data['category1'].unique())}\n",
    "# print(label_to_id)\n",
    "# train_data['label'] = train_data['category1'].map(label_to_id)\n",
    "# test_data['label'] = test_data['category1'].map(label_to_id)\n",
    "# num_classes = len(label_to_id)\n",
    "# print(num_classes)\n",
    "# # 将 'label' 列转换为字符串类型\n",
    "# train_str = train_data['label'].astype(str)\n",
    "# test_str = test_data['label'].astype(str)\n",
    "\n",
    "# # 完整输出 DataFrame\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     print(train_str)\n",
    "#     print(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53fb0e5e-8e99-4ee0-9e4b-be0f5dd0dd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=33, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        # label = str(self.labels[idx]) # todo: 改str试试\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long) \n",
    "        }\n",
    "    \n",
    "    \n",
    "# 创建训练和测试数据集实例\n",
    "train_dataset = CustomDataset(train_data['body'].values, train_data['label'].values, tokenizer)\n",
    "test_dataset = CustomDataset(test_data['body'].values, test_data['label'].values, tokenizer)\n",
    "\n",
    "# 使用DataLoader加载数据\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) \n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "#2.20 修改batchsize\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) \n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 将模型移动到GPU上（如果可用）\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ad3c12-1610-4878-a50c-b2868e05abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器和损失函数\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5) # 优化器可调整 学习率可调整\n",
    "# optimizer = torch.optim.AdamW(model.parameters(),  lr=2e-5) # 修改新用法  优化器可调整 学习率可调整\n",
    "optimizer = torch.optim.AdamW(model.parameters(),  lr=5e-5) # 修改新用法  优化器可调整 学习率可调整 2.20 5e-5\n",
    "criterion = torch.nn.CrossEntropyLoss() # 损失函数可调整\n",
    "\n",
    "# 定义训练函数\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with tqdm(loader, desc=\"Training\", leave=False) as iterator:\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b4082a-b62b-48bb-a698-4268f9dd4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义评估函数\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # 修改返回val_loss\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad(), tqdm(loader, desc=\"Evaluating\", leave=False) as iterator:\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "            # 修改返回val_loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * len(labels)\n",
    "            num_samples += len(labels)\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 修改返回val_loss\n",
    "    avg_loss = total_loss / num_samples\n",
    "    return all_labels, all_preds, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d96f1eb5-a4da-4b65-8d60-c5274eb3cdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/13 [00:00<?, ?it/s]/opt/conda/envs/default/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train Loss: 2.658506906949557, Val Loss: 2.634661388397217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Train Loss: 2.505264025468093, Val Loss: 2.4872194004058836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    t1,t2,val_loss = evaluate(model, test_loader, device)  # 假设evaluate函数用于计算验证集损失\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "    # 在每个 epoch 结束时记录训练损失和验证损失\n",
    "    logging.info(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d378e427-f72d-4802-93a6-329c54c57db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]/opt/conda/envs/default/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 17, 16, 28, 26, 29, 28, 28, 17, 17, 17, 10, 28, 32, 30, 31, 16, 31, 17, 15, 27, 17, 17, 28, 16, 15, 17, 17, 31, 31, 17, 17, 28, 27, 17, 16, 29, 28, 31, 4, 15, 17, 15, 32, 17, 15, 30, 28, 24, 17, 28, 31, 19, 28, 0, 1, 27, 1, 15, 30, 17, 1, 27, 16, 28, 16, 28, 17, 11, 27, 17, 28, 28, 16, 17, 16, 28, 1, 28, 15, 29, 9, 14, 9, 15, 27, 17, 15, 17, 15, 21, 22, 28, 15, 15, 24, 16, 15, 8, 17]\n",
      "[28, 17, 28, 28, 17, 17, 28, 28, 17, 28, 17, 17, 28, 17, 17, 17, 17, 17, 28, 17, 28, 28, 28, 28, 17, 17, 28, 28, 28, 17, 28, 17, 28, 17, 17, 28, 28, 28, 17, 17, 17, 17, 17, 17, 17, 17, 17, 28, 0, 17, 28, 17, 17, 28, 17, 17, 17, 17, 28, 17, 17, 17, 17, 17, 28, 17, 28, 28, 17, 17, 28, 28, 28, 17, 17, 28, 28, 17, 28, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 22, 0, 28, 17, 17, 0, 28, 28, 17, 28]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      1.00         1\n",
      "           1       1.00      0.00      0.00         6\n",
      "           2       1.00      0.00      0.00         1\n",
      "           4       1.00      0.00      0.00         1\n",
      "           5       1.00      0.00      0.00         1\n",
      "           8       1.00      0.00      0.00         1\n",
      "           9       1.00      0.00      0.00         2\n",
      "          10       1.00      0.00      0.00         1\n",
      "          11       1.00      0.00      0.00         1\n",
      "          12       1.00      0.00      0.00         1\n",
      "          14       1.00      0.00      0.00         3\n",
      "          15       1.00      0.00      0.00        29\n",
      "          16       1.00      0.00      0.00        17\n",
      "          17       0.19      0.56      0.28        39\n",
      "          18       1.00      0.00      0.00         2\n",
      "          19       1.00      0.00      0.00         5\n",
      "          21       1.00      0.00      0.00         2\n",
      "          22       0.00      0.00      1.00         1\n",
      "          23       1.00      0.00      0.00         2\n",
      "          24       1.00      0.00      0.00         4\n",
      "          26       1.00      0.00      0.00         3\n",
      "          27       1.00      0.00      0.00        14\n",
      "          28       0.39      0.97      0.55        30\n",
      "          29       1.00      0.00      0.00         6\n",
      "          30       0.00      0.00      1.00         9\n",
      "          31       1.00      0.00      0.00        11\n",
      "          32       1.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.26       200\n",
      "   macro avg       0.84      0.06      0.14       200\n",
      "weighted avg       0.69      0.26      0.19       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# 评估模型\n",
    "true_labels, predicted_labels,test_loss = evaluate(model, test_loader, device)\n",
    "print(true_labels[:100])\n",
    "print(predicted_labels[:100])\n",
    "# print(true_labels)\n",
    "# print(predicted_labels)\n",
    "\n",
    "# print(classification_report(true_labels, predicted_labels))\n",
    "print(classification_report(true_labels, predicted_labels,zero_division=1))\n",
    "logging.info(classification_report(true_labels, predicted_labels,zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ad37c0b-8db0-430b-8530-2b223115bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"./models/{model_name}_model_test_epoch_{num_epochs}.pth\"\n",
    "\n",
    "# 保存模型到文件\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# # 保存模型到文件\n",
    "# torch.save(model.state_dict(), 'bert_model_test_e.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ba39e-187b-4f1f-a291-55b705f8c06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
