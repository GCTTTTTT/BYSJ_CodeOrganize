{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9587672-cd27-47b6-b4ca-fba021d34f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a965b023-d949-43ff-841a-389b3ef5672d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records before deduplication: 1663846\n",
      "Total records after deduplication: 1661972\n"
     ]
    }
   ],
   "source": [
    "# 语料去重 （可运行）\n",
    "\n",
    "# 读取CSV文件\n",
    "# file_path = 'mangoNews_Example.csv'  # 测试版数据集（规模小5m）\n",
    "file_path = 'datasets/mangoNews.csv'          # 完整版数据集（13g） 需要分块读\n",
    "# file_path = 'datasets/mangoNews_Example_100000.csv'          # 100000行数据集（2.2g）\n",
    "# file_path = 'mangoNews_Example_10000.csv'          # 10000行数据集（190m)\n",
    "\n",
    "data = pd.read_csv(file_path,low_memory=False,lineterminator=\"\\n\")\n",
    "\n",
    "# 统计去重前的总语料数\n",
    "total_records_before = len(data)\n",
    "\n",
    "# 根据多个列进行去重，并保留重复行\n",
    "# duplicates = data[data.duplicated(subset=[\"website_id\", \"request_url\",\"response_url\",\"category1\", \"category2\", \"title\", \"body\",\"pub_time\"], keep=False)]\n",
    "\n",
    "# 输出重复行\n",
    "# print(\"Duplicate records:\")\n",
    "# print(duplicates)\n",
    "\n",
    "# 对 'body'列内容都重复的语料进行去重处理\n",
    "# data.drop_duplicates(subset=['body'], inplace=True)\n",
    "# data.drop_duplicates(subset=[\"category1\", \"category2\", \"title\", \"body\"], inplace=True) # 100025  99352\n",
    "# data.drop_duplicates(subset=[\"website_id\", \"request_url\", \"category1\", \"category2\", \"title\", \"body\"], inplace=True) # 100025  99534\n",
    "# data.drop_duplicates(subset=[\"website_id\", \"request_url\",\"response_url\",\"category1\", \"category2\", \"title\", \"body\"], inplace=True) # 100025  99534\n",
    "# data.drop_duplicates(subset=[\"website_id\", \"request_url\",\"response_url\",\"category1\", \"category2\", \"title\", \"body\",\"pub_time\"], inplace=True) # 100025  99829\n",
    "# data.drop_duplicates(subset=[\"website_id\", \"request_url\",\"response_url\",\"category1\", \"category2\", \"title\", \"body\",\"pub_time\",\"cole_time\"], inplace=True) # 100025  100019\n",
    "# data.drop_duplicates(subset=[\"website_id\", \"request_url\",\"response_url\",\"category1\", \"category2\", \"title\", \"body\",\"pub_time\",\"cole_time\",\"images\"], inplace=True) # 100025  100022\n",
    "# 不同时间爬到同一篇（除爬取时间和id外其他相同）\n",
    "data.drop_duplicates(subset=[\"website_id\", \"request_url\",\"response_url\",\"category1\", \"category2\", \"title\", \"abstract\",\"body\",\"pub_time\",\"images\"], inplace=True) # 100025  99862\n",
    "\n",
    "# data.drop_duplicates(subset=[\"md5\"], inplace=True) # 100025  99474\n",
    "\n",
    "\n",
    "# 统计去重后的总语料数\n",
    "total_records_after = len(data)\n",
    "\n",
    "data.to_csv('datasets_FIX/FIX_deduplicated_mangoNews.csv', index=False)\n",
    "\n",
    "\n",
    "# 打印统计结果\n",
    "print(f\"Total records before deduplication: {total_records_before}\")\n",
    "print(f\"Total records after deduplication: {total_records_after}\")\n",
    "# 10000：10007 9969\n",
    "# 100000:100025 98912\n",
    "\n",
    "# new tragedy： 1663846 1661972 !!! new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ee640-aed7-4769-8dd8-9609805e79a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100000\n",
      "1 100000\n",
      "2 100000\n",
      "3 100000\n",
      "4 100000\n",
      "5 100000\n",
      "6 100000\n",
      "7 100000\n",
      "8 100000\n",
      "9 100000\n",
      "10 100000\n",
      "11 100000\n",
      "12 100000\n",
      "13 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5413/1571218463.py:20: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 100000\n",
      "15 100000\n",
      "16 63846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5413/1571218463.py:37: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk_deduped = pd.read_csv(temp_file)\n",
      "/tmp/ipykernel_5413/1571218463.py:37: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk_deduped = pd.read_csv(temp_file)\n",
      "/tmp/ipykernel_5413/1571218463.py:37: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk_deduped = pd.read_csv(temp_file)\n"
     ]
    }
   ],
   "source": [
    "# 语料去重 大语料分块处理 （别运行这个，虽然有整体去重的操作，但对于完整的数据集会爆）\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 读取CSV文件，使用chunksize进行分块读取\n",
    "file_path = 'mangoNews.csv'          # 完整版数据集（13g） 需要分块读\n",
    "# file_path = 'mangoNews_Example_100000.csv'          # 100000行数据集（2.2g）\n",
    "# file_path = 'mangoNews_Example_10000.csv'          # 10000行数据集（190m)\n",
    "\n",
    "chunk_size = 100000  # 根据实际情况调整分块大小\n",
    "temp_files = []\n",
    "\n",
    "# 初始化空的DataFrame，用于存储全局去重后的数据\n",
    "data_deduped = pd.DataFrame(columns=['body'])\n",
    "\n",
    "total_records_before2 = 0\n",
    "total_records_after2 = 0\n",
    "\n",
    "# 遍历分块数据\n",
    "for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size, low_memory=False)):\n",
    "    # 统计当前分块的记录数并累加到总记录数\n",
    "    print(i,len(chunk))\n",
    "    total_records_before2 += len(chunk)\n",
    "    \n",
    "    # 对当前分块数据进行去重操作\n",
    "    chunk.drop_duplicates(subset=['body'], inplace=True)\n",
    "\n",
    "    # 将当前分块数据写入临时文件\n",
    "    temp_file_path = f'temp_chunk_{i}.csv'\n",
    "    chunk.to_csv(temp_file_path, index=False)\n",
    "    temp_files.append(temp_file_path)\n",
    "    \n",
    "    \n",
    "\n",
    "# 读取并合并所有临时文件的数据\n",
    "for temp_file in temp_files:\n",
    "    chunk_deduped = pd.read_csv(temp_file)\n",
    "    data_deduped = pd.concat([data_deduped, chunk_deduped], ignore_index=True)\n",
    "\n",
    "# 对合并后的 DataFrame 再次进行全局的去重操作\n",
    "data_deduped.drop_duplicates(subset=['body'], inplace=True)\n",
    "\n",
    "# 删除临时文件\n",
    "for temp_file in temp_files:\n",
    "    os.remove(temp_file)\n",
    "\n",
    "# 统计去重前后的总语料数\n",
    "# total_records_before = len(data)\n",
    "total_records_after2 = len(data_deduped)\n",
    "\n",
    "# 打印统计结果\n",
    "print(f\"Total records before deduplication: {total_records_before2}\")\n",
    "print(f\"Total records after deduplication: {total_records_after2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591f2e67-a5d1-44b4-a2f6-5eef210b4ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 300000\n",
      "1 300000\n",
      "2 300000\n",
      "3 300000\n",
      "4 300000\n",
      "5 163846\n",
      "Total records before deduplication: 1663846\n",
      "Total records after deduplication: 1347969\n"
     ]
    }
   ],
   "source": [
    "# 爆存 退而求其次 （可运行）\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件，使用chunksize进行分块读取\n",
    "file_path = 'mangoNews.csv'          # 完整版数据集（13g） 需要分块读\n",
    "# file_path = 'mangoNews_Example_100000.csv'          # 100000行数据集（2.2g）\n",
    "# file_path = 'mangoNews_Example_10000.csv'          # 10000行数据集（190m)\n",
    "\n",
    "chunk_size = 300000  # 根据实际情况调整分块大小\n",
    "data_chunks = pd.read_csv(file_path, chunksize=chunk_size, low_memory=False)\n",
    "\n",
    "# 初始化空的 DataFrame，用于存储去重后的数据\n",
    "deduplicated_data = pd.DataFrame(columns=['body'])\n",
    "\n",
    "total_records_before3 = 0\n",
    "total_records_after3 = 0\n",
    "\n",
    "# # 将去重后的数据写入新的CSV文件\n",
    "deduplicated_file_path = 'deduplicated_mangoNews.csv'  # 新CSV文件路径\n",
    "\n",
    "# 遍历分块数据\n",
    "for i, chunk in enumerate(data_chunks):\n",
    "    # 统计当前分块的记录数\n",
    "    total_records_before3 += len(chunk)\n",
    "    print(i,len(chunk))\n",
    "\n",
    "    # 对 'body'列内容都重复的语料进行去重处理\n",
    "    chunk.drop_duplicates(subset=['body'], inplace=True)\n",
    "\n",
    "    # 统计去重后的总语料数\n",
    "    total_records_after3 += len(chunk)\n",
    "    \n",
    "    # 将去重后的数据追加到同一个 CSV 文件中\n",
    "    mode = 'w' if i == 0 else 'a'  # 第一个分块使用 'w' 模式，后续分块使用 'a' 模式追加数据\n",
    "    header = False if i != 0 else True  # 只在第一个分块写入时写入表头\n",
    "    \n",
    "    chunk.to_csv(deduplicated_file_path, index=False, mode=mode, header=header)\n",
    "\n",
    "\n",
    "\n",
    "# deduplicated_data.to_csv(deduplicated_file_path, index=False)\n",
    "\n",
    "# print(\"test\")\n",
    "# print(deduplicated_data.shape[0])\n",
    "# print(len(deduplicated_data))\n",
    "\n",
    "# 打印统计结果\n",
    "print(f\"Total records before deduplication: {total_records_before3}\")\n",
    "print(f\"Total records after deduplication: {total_records_after3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e5fc2f4-d733-4257-af13-3023fb516f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Counts:\n",
      "              category1     num\n",
      "0                 সংবাদ  414224\n",
      "1              বাংলাদেশ  126016\n",
      "2         আজকের পত্রিকা   94927\n",
      "3           আন্তর্জাতিক   75581\n",
      "4                 জাতীয়   73246\n",
      "..                  ...     ...\n",
      "789                   এ       1\n",
      "790               ভারত        1\n",
      "791            আবু রুশদ       1\n",
      "792         জগদীশ গুপ্ত       1\n",
      "793  সুকুমার ভট্টাচার্য       1\n",
      "\n",
      "[794 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 'mangoNews_10000.csv'和'mangoNews_100000.csv'中类别统计并保存\n",
    "# 读取CSV文件\n",
    "# file_path = 'mangoNews_Example.csv'  # 测试版数据集（规模小5m）\n",
    "# file_path = 'datasets/mangoNews.csv'          # 完整版数据集（13g） 需要分块读\n",
    "# file_path = 'mangoNews_Example_100000.csv'          # 100000行数据集（2.2g）\n",
    "# file_path = 'mangoNews_Example_10000.csv'          # 10000行数据集（190m)\n",
    "\n",
    "file_path = 'datasets_FIX/FIX_deduplicated_mangoNews.csv'          # 完整版数据集（13g） 需要分块读\n",
    "\n",
    "data = pd.read_csv(file_path,low_memory=False,lineterminator=\"\\n\")\n",
    "\n",
    "# Select relevant columns\n",
    "data = data[['body', 'category1']]\n",
    "\n",
    "# 统计 'category1' 列中每种类别的个数\n",
    "category_counts = data['category1'].value_counts()\n",
    "\n",
    "# # 设置显示选项，完整输出结果\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# print(\"Category Counts:\")\n",
    "# print(category_counts)\n",
    "# # 恢复默认显示选项\n",
    "# pd.reset_option('display.max_rows')\n",
    "\n",
    "# 将结果保存到CSV文件\n",
    "result_df = pd.DataFrame({'category1': category_counts.index, 'num': category_counts.values})\n",
    "# result_df.to_csv('res_files_FIX/mangoNews_category_counts_all.csv', index=False)\n",
    "result_df.to_csv('res_files_FIX/FIX_deduplicated_mangoNews_category_counts_all.csv', index=False)\n",
    "\n",
    "\n",
    "# 打印结果\n",
    "print(\"Category Counts:\")\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80717fa4-f597-405a-bc67-23e2af65447b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Counts (Sorted):\n",
      "          category1     num\n",
      "701           সংবাদ  415412\n",
      "493        বাংলাদেশ  126064\n",
      "166   আজকের পত্রিকা   94927\n",
      "173     আন্তর্জাতিক   75581\n",
      "319           জাতীয়   73246\n",
      "..              ...     ...\n",
      "555     মঞ্জু সরকার       1\n",
      "153     অরূপরতন ঘোষ       1\n",
      "152  অরুণকুমার দত্ত       1\n",
      "561        মনোজ বসু       1\n",
      "793            ২০২০       1\n",
      "\n",
      "[794 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 'mangoNews.csv'中类别统计并保存\n",
    "# 读取CSV文件\n",
    "# file_path = 'mangoNews_Example.csv'  # 测试版数据集（规模小5m）\n",
    "file_path = 'mangoNews.csv'          # 完整版数据集（13g） 需要分块读\n",
    "# file_path = 'mangoNews_Example_100000.csv'          # 100000行数据集（2.2g）\n",
    "# file_path = 'mangoNews_Example_10000.csv'          # 10000行数据集（190m)\n",
    "\n",
    "# 分块读取：\n",
    "# 读取CSV文件，使用chunksize进行分块读取\n",
    "# file_path = 'your_dataset.csv'  # 替换为你的数据集路径\n",
    "chunk_size = 100000  # 根据实际情况调整分块大小\n",
    "data_chunks = pd.read_csv(file_path, usecols=['body', 'category1'], chunksize=chunk_size)\n",
    "\n",
    "# 初始化空的DataFrame，用于存储统计结果\n",
    "result_df = pd.DataFrame(columns=['category1', 'num'])\n",
    "\n",
    "# 遍历分块数据\n",
    "for chunk in data_chunks:\n",
    "    # 统计 'category1' 列中每种类别的个数\n",
    "    category_counts = chunk['category1'].value_counts().reset_index()\n",
    "    category_counts.columns = ['category1', 'num']\n",
    "\n",
    "    # 合并到总的结果DataFrame\n",
    "    result_df = pd.concat([result_df, category_counts], ignore_index=True)\n",
    "\n",
    "# 合并结果中的重复项，得到最终的统计结果\n",
    "final_result = result_df.groupby('category1')['num'].sum().reset_index()\n",
    "\n",
    "# 按照 'num' 列降序排序\n",
    "final_result = final_result.sort_values(by='num', ascending=False)\n",
    "\n",
    "# 将最终结果保存到CSV文件\n",
    "final_result.to_csv('category_counts_all.csv', index=False)\n",
    "\n",
    "# 打印结果\n",
    "# 设置显示选项，完整输出结果\n",
    "# pd.set_option('display.max_rows', None)\n",
    "print(\"Category Counts (Sorted):\")\n",
    "print(final_result)\n",
    "# 恢复默认显示选项\n",
    "# pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67b46b-cb4d-4532-a913-b41e37a12852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将类别列转换为整数标签  注意是data['category1']\n",
    "label_to_id = {label: idx for idx, label in enumerate(data['category1'].unique())}\n",
    "print(label_to_id)\n",
    "data['label'] = data['category1'].map(label_to_id)\n",
    "num_classes = len(label_to_id)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3093e1-2d4d-4ad6-8782-fb3d8b5503e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-default",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
