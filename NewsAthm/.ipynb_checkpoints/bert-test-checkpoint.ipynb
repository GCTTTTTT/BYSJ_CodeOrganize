{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f288323c-e4d3-44eb-ac2f-02f775712439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08b61fb7-3ff8-4352-b359-1c7b000b8b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Counts:\n",
      "category1\n",
      "কাজী নজরুল ইসলাম                203\n",
      "সুকুমার রায়                    145\n",
      "মাইকেল মধুসূদন দত্ত             133\n",
      "জসীমউদ্দীন                       96\n",
      "হেলাল হাফিজ                      59\n",
      "সুনীল গঙ্গোপাধ্যায়               52\n",
      "আবিদ আনোয়ার                     41\n",
      "writer                           41\n",
      "রবীন্দ্রনাথ ঠাকুর                33\n",
      "সত্যেন্দ্রনাথ দত্ত               21\n",
      "সুফিয়া কামাল                     20\n",
      "Onubad - Copy                    20\n",
      "Front Page-Detective Fiction     18\n",
      "কামিনী রায়                       17\n",
      "Front Page-Short Stories         16\n",
      "amader-kotha                     14\n",
      "Front Page-Poetry                14\n",
      "Front Page-Science Fiction       13\n",
      "Front Page-Tales of Unease       10\n",
      "ঈশ্বরচন্দ্র গুপ্ত                 8\n",
      "নারী ও সমাজ                       4\n",
      "Front Page-Autobiography          3\n",
      "বিজ্ঞান ও প্রযুক্তি               3\n",
      "বই-টই                             3\n",
      "স্মরণে                            3\n",
      "ব্যক্তিগত গদ্য                    2\n",
      "শিল্প- সংস্কৃতি                   2\n",
      "রম্য রচনা                         1\n",
      "ভ্রমণ                             1\n",
      "সাহিত্য                           1\n",
      "ক্রীড়াজগৎ                         1\n",
      "দর্শন- রাজনীতি- ইতিহাস            1\n",
      "সম্পাদকীয়                         1\n",
      "Name: count, dtype: int64\n",
      "{'amader-kotha': 0, 'writer': 1, 'সম্পাদকীয়': 2, 'বই-টই': 3, 'দর্শন- রাজনীতি- ইতিহাস': 4, 'ক্রীড়াজগৎ': 5, 'রম্য রচনা': 6, 'ভ্রমণ': 7, 'ব্যক্তিগত গদ্য': 8, 'স্মরণে': 9, 'নারী ও সমাজ': 10, 'শিল্প- সংস্কৃতি': 11, 'বিজ্ঞান ও প্রযুক্তি': 12, 'সাহিত্য': 13, 'ঈশ্বরচন্দ্র গুপ্ত': 14, 'সুকুমার রায়': 15, 'জসীমউদ্দীন': 16, 'কাজী নজরুল ইসলাম': 17, 'কামিনী রায়': 18, 'সত্যেন্দ্রনাথ দত্ত': 19, 'Front Page-Autobiography': 20, 'Front Page-Tales of Unease': 21, 'Front Page-Short Stories': 22, 'Front Page-Science Fiction': 23, 'Front Page-Detective Fiction': 24, 'Front Page-Poetry': 25, 'Onubad - Copy': 26, 'হেলাল হাফিজ': 27, 'মাইকেল মধুসূদন দত্ত': 28, 'সুফিয়া কামাল': 29, 'সুনীল গঙ্গোপাধ্যায়': 30, 'আবিদ আনোয়ার': 31, 'রবীন্দ্রনাথ ঠাকুর': 32}\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "# 读取CSV文件\n",
    "file_path = 'mangoNews_Example.csv'  # 测试版数据集（规模小5m）\n",
    "# file_path = 'mangoNews.csv'          # 完整版数据集（13g） 需要分块读\n",
    "# file_path = 'mangoNews_Example_100000.csv'          # 100000行数据集（2.2g）\n",
    "# file_path = 'mangoNews_Example_10000.csv'          # 10000行数据集（190m)\n",
    "# file_path = './deduplicated_mangoNews_Nums3000p_CategoryMerge.csv'          # 去重后保留类别对应语料数在3000+并合并同义类别的数据集（9.9g) 需要分块读\n",
    "# file_path = './deduplicated_mangoNews_Nums3000p_CategoryMerge_100000_1.csv'          # 去重后保留类别对应语料数在3000+并合并同义类别的数据集（576m) \n",
    "# file_path = './deduplicated_mangoNews_Nums3000p_CategoryMerge_990000.csv'          # 去重后保留类别对应语料数在3000+并合并同义类别的数据集（8.1g) \n",
    "\n",
    "data = pd.read_csv(file_path,low_memory=False,lineterminator=\"\\n\")\n",
    "\n",
    "# Select relevant columns\n",
    "data = data[['body', 'category1']]\n",
    "\n",
    "# 统计 'category1' 列中每种类别的个数\n",
    "category_counts = data['category1'].value_counts()\n",
    "\n",
    "# 设置显示选项，完整输出结果\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(\"Category Counts:\")\n",
    "print(category_counts)\n",
    "# 恢复默认显示选项\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "# 将类别列转换为整数标签  注意是data['category1']\n",
    "label_to_id = {label: idx for idx, label in enumerate(data['category1'].unique())}\n",
    "print(label_to_id)\n",
    "data['label'] = data['category1'].map(label_to_id)\n",
    "num_classes = len(label_to_id)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d56cbf21-47d8-4551-8156-c6e74f7b34fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'a', 'good', 'time', ',', 'thank', 'you', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./uncased_L-12_H-768_A-12 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: 17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Bert_path = './uncased_L-12_H-768_A-12'  # bert-base-uncased from github\n",
    "\n",
    "# 初始化Bert tokenizer和模型\n",
    "tokenizer = BertTokenizer.from_pretrained(Bert_path) # bert: base or large（wwm&origin） \n",
    "\n",
    "print(tokenizer.tokenize('I have a good time, thank you.')) # 测试\n",
    "\n",
    "# 创建一个新的模型实例\n",
    "model = BertForSequenceClassification.from_pretrained(Bert_path, num_labels=num_classes)\n",
    "\n",
    "# 加载之前保存的模型参数\n",
    "model.load_state_dict(torch.load('bert_model.pth'))\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 加载待推断的数据\n",
    "text = \"জ্যোৎস্নার গান ভুলে যাওয়া অন্ধ মানুষ – স্বপ্ন ওড়ে – নিদ্রিত দুপুরে,নিঃস্বতার তামাশার রঙিন ফানুস ছিঁড়ে যায় উজ্জ্বল তিমিরে;রাত্রি জেগে থাকে কুয়াশার ঘোরলাগা দূর একাত্তরে,বিদীর্ণ পাঁজরে – মৃত্যু আর –বিষণ্ন রুগ্ণ ছায়ার ভিড়ে;জন্মান্ধ রাত্রির সিঁড়ির নিচে নিবিড় নির্জনেভুল ওড়ে – মন পোড়ে –পৃথিবীর প্রিয় ফুলগুলি যায় ঝরেময়লাদিনের গূঢ় গহিন আস্তিনে;দুর্দিনের শীতে কী যে বধির বিষাদঅন্ধ দূর দেখে জীবনের ক্ষয়ক্ষতি  –প্রফুল্ল ভোরের স্বপ্ন-সাধকরতলে ভাঙে প্রতিশ্রুতি;সন্ধেগুলো শীতরাত্রির ভ্রমণে গেলেদূর নক্ষত্রেরা যাবে কী গোপন ভুলে!\"\n",
    "\n",
    "# 对文本进行预处理和转换\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "\n",
    "# 使用模型进行预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "# 打印预测结果\n",
    "print(\"Predicted category:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e639f81-d734-458b-aedf-0b3c82533717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Counts:\n",
      "category1\n",
      "কাজী নজরুল ইসলাম                203\n",
      "সুকুমার রায়                    145\n",
      "মাইকেল মধুসূদন দত্ত             133\n",
      "জসীমউদ্দীন                       96\n",
      "হেলাল হাফিজ                      59\n",
      "সুনীল গঙ্গোপাধ্যায়               52\n",
      "আবিদ আনোয়ার                     41\n",
      "writer                           41\n",
      "রবীন্দ্রনাথ ঠাকুর                33\n",
      "সত্যেন্দ্রনাথ দত্ত               21\n",
      "সুফিয়া কামাল                     20\n",
      "Onubad - Copy                    20\n",
      "Front Page-Detective Fiction     18\n",
      "কামিনী রায়                       17\n",
      "Front Page-Short Stories         16\n",
      "amader-kotha                     14\n",
      "Front Page-Poetry                14\n",
      "Front Page-Science Fiction       13\n",
      "Front Page-Tales of Unease       10\n",
      "ঈশ্বরচন্দ্র গুপ্ত                 8\n",
      "নারী ও সমাজ                       4\n",
      "Front Page-Autobiography          3\n",
      "বিজ্ঞান ও প্রযুক্তি               3\n",
      "বই-টই                             3\n",
      "স্মরণে                            3\n",
      "ব্যক্তিগত গদ্য                    2\n",
      "শিল্প- সংস্কৃতি                   2\n",
      "রম্য রচনা                         1\n",
      "ভ্রমণ                             1\n",
      "সাহিত্য                           1\n",
      "ক্রীড়াজগৎ                         1\n",
      "দর্শন- রাজনীতি- ইতিহাস            1\n",
      "সম্পাদকীয়                         1\n",
      "Name: count, dtype: int64\n",
      "{'amader-kotha': 0, 'writer': 1, 'সম্পাদকীয়': 2, 'বই-টই': 3, 'দর্শন- রাজনীতি- ইতিহাস': 4, 'ক্রীড়াজগৎ': 5, 'রম্য রচনা': 6, 'ভ্রমণ': 7, 'ব্যক্তিগত গদ্য': 8, 'স্মরণে': 9, 'নারী ও সমাজ': 10, 'শিল্প- সংস্কৃতি': 11, 'বিজ্ঞান ও প্রযুক্তি': 12, 'সাহিত্য': 13, 'ঈশ্বরচন্দ্র গুপ্ত': 14, 'সুকুমার রায়': 15, 'জসীমউদ্দীন': 16, 'কাজী নজরুল ইসলাম': 17, 'কামিনী রায়': 18, 'সত্যেন্দ্রনাথ দত্ত': 19, 'Front Page-Autobiography': 20, 'Front Page-Tales of Unease': 21, 'Front Page-Short Stories': 22, 'Front Page-Science Fiction': 23, 'Front Page-Detective Fiction': 24, 'Front Page-Poetry': 25, 'Onubad - Copy': 26, 'হেলাল হাফিজ': 27, 'মাইকেল মধুসূদন দত্ত': 28, 'সুফিয়া কামাল': 29, 'সুনীল গঙ্গোপাধ্যায়': 30, 'আবিদ আনোয়ার': 31, 'রবীন্দ্রনাথ ঠাকুর': 32}\n",
      "33\n",
      "before\n",
      "No NaN values in the 'category1' column.\n",
      "after\n",
      "No NaN values in the 'category1' column.\n",
      "['i', 'have', 'a', 'good', 'time', ',', 'thank', 'you', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./uncased_L-12_H-768_A-12 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/11 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train Loss: 3.175054528496482, Val Loss: 2.998176431655884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Train Loss: 2.8942238200794566, Val Loss: 2.8087137190500897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 17, 16, 28, 26, 29, 28, 28, 17, 17, 17, 10, 28, 32, 30, 31, 16, 31, 17, 15, 27, 17, 17, 28, 16, 15, 17, 17, 31, 31, 17, 17, 28, 27, 17, 16, 29, 28, 31, 4, 15, 17, 15, 32, 17, 15, 30, 28, 24, 17, 28, 31, 19, 28, 0, 1, 27, 1, 15, 30, 17, 1, 27, 16, 28, 16, 28, 17, 11, 27, 17, 28, 28, 16, 17, 16, 28, 1, 28, 15, 29, 9, 14, 9, 15, 27, 17, 15, 17, 15, 21, 22, 28, 15, 15, 24, 16, 15, 8, 17]\n",
      "[17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00         6\n",
      "           1       1.00      0.00      0.00        10\n",
      "           2       1.00      0.00      0.00         1\n",
      "           4       1.00      0.00      0.00         1\n",
      "           5       1.00      0.00      0.00         1\n",
      "           8       1.00      0.00      0.00         1\n",
      "           9       1.00      0.00      0.00         3\n",
      "          10       1.00      0.00      0.00         2\n",
      "          11       1.00      0.00      0.00         1\n",
      "          12       1.00      0.00      0.00         1\n",
      "          14       1.00      0.00      0.00         3\n",
      "          15       1.00      0.00      0.00        42\n",
      "          16       1.00      0.00      0.00        31\n",
      "          17       0.18      1.00      0.31        54\n",
      "          18       1.00      0.00      0.00         4\n",
      "          19       1.00      0.00      0.00         7\n",
      "          21       1.00      0.00      0.00         4\n",
      "          22       1.00      0.00      0.00         1\n",
      "          23       1.00      0.00      0.00         3\n",
      "          24       1.00      0.00      0.00         7\n",
      "          25       1.00      0.00      0.00         1\n",
      "          26       1.00      0.00      0.00         4\n",
      "          27       1.00      0.00      0.00        17\n",
      "          28       1.00      0.00      0.00        44\n",
      "          29       1.00      0.00      0.00         6\n",
      "          30       1.00      0.00      0.00        18\n",
      "          31       1.00      0.00      0.00        16\n",
      "          32       1.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.18       300\n",
      "   macro avg       0.97      0.04      0.01       300\n",
      "weighted avg       0.85      0.18      0.05       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# undersampled 欠采样平衡后数据进行训练 ephoch=10\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "def reset_log(log_path):\n",
    "    import logging\n",
    "    fileh = logging.FileHandler(log_path, 'a')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fileh.setFormatter(formatter)\n",
    "    log = logging.getLogger()  # root logger\n",
    "    for hdlr in log.handlers[:]:  # remove all old handlers\n",
    "        log.removeHandler(hdlr)\n",
    "    log.addHandler(fileh)\n",
    "    log.setLevel(logging.INFO)\n",
    "\n",
    "reset_log('./logs/bert-classification-server-undersampled_training.log')\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.info('This is a log info')\n",
    "\n",
    "# # 配置日志记录器\n",
    "# logging.basicConfig(filename='./logs/bert-classification-server-undersampled_training.log', level=logging.INFO, format='%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S') ## \n",
    "\n",
    "\n",
    "\n",
    "# 读取CSV文件\n",
    "file_path = './datasets/mangoNews_Example.csv'  # 测试版数据集（规模小5m）\n",
    "# file_path = './datasets/mangoNews.csv'          # 完整版数据集（13g） 需要分块读\n",
    "# file_path = './datasets/mangoNews_Example_100000.csv'          # 100000行数据集（2.2g）\n",
    "# file_path = './datasets/mangoNews_Example_10000.csv'          # 10000行数据集（190m)\n",
    "# file_path = './datasets/deduplicated_mangoNews_Nums3000p_CategoryMerge.csv'          # 去重后保留类别对应语料数在3000+并合并同义类别的数据集（9.9g) 需要分块读\n",
    "# file_path = './datasets/deduplicated_mangoNews_Nums3000p_CategoryMerge_100000_1.csv'          # 去重后保留类别对应语料数在3000+并合并同义类别的数据集（576m) \n",
    "# file_path = './datasets/deduplicated_mangoNews_Nums3000p_CategoryMerge_990000.csv'          # 去重后保留类别对应语料数在3000+并合并同义类别的数据集（8.1g) \n",
    "# file_path = './datasets/deduplicated_mangoNews_Nums3000p_CategoryMerge_new_undersampled.csv'          # 去重后保留类别对应语料数在3000+并合并同义类别(新）并随机欠采样数据平衡的数据集（12095*12条 1.2g) \n",
    "\n",
    "\n",
    "data = pd.read_csv(file_path,low_memory=False,lineterminator=\"\\n\")\n",
    "\n",
    "# Select relevant columns\n",
    "data = data[['body', 'category1']]\n",
    "\n",
    "# 统计 'category1' 列中每种类别的个数\n",
    "category_counts = data['category1'].value_counts()\n",
    "\n",
    "# 设置显示选项，完整输出结果\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(\"Category Counts:\")\n",
    "print(category_counts)\n",
    "# 恢复默认显示选项\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "# 将类别列转换为整数标签  注意是data['category1']\n",
    "label_to_id = {label: idx for idx, label in enumerate(data['category1'].unique())}\n",
    "print(label_to_id)\n",
    "data['label'] = data['category1'].map(label_to_id)\n",
    "num_classes = len(label_to_id)\n",
    "print(num_classes)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42) ## 2.20 test_size:0.2->0.3\n",
    "\n",
    "# 检验 'category1' 列是否还有 NaN 值\n",
    "# nan_check = train_data['category1'].isnull().sum()\n",
    "nan_check = train_data['category1'].isna().sum()\n",
    "\n",
    "print(\"before\")\n",
    "if nan_check > 0:\n",
    "    print(f\"There are still {nan_check} NaN values in the 'category1' column.\")\n",
    "else:\n",
    "    print(\"No NaN values in the 'category1' column.\")\n",
    "    \n",
    "# 去除包含缺失值的样本\n",
    "train_data = train_data.dropna(subset=['category1'])\n",
    "test_data = test_data.dropna(subset=['category1'])\n",
    "# todo：注意后面要提取缺失值\n",
    "\n",
    "print(\"after\")\n",
    "# 检验 'category1' 列是否还有 NaN 值\n",
    "# nan_check = train_data['category1'].isnull().sum()\n",
    "nan_check = train_data['category1'].isna().sum()\n",
    "\n",
    "if nan_check > 0:\n",
    "    print(f\"There are still {nan_check} NaN values in the 'category1' column.\")\n",
    "else:\n",
    "    print(\"No NaN values in the 'category1' column.\")\n",
    "\n",
    "Bert_path = './uncased_L-12_H-768_A-12'  # bert-base-uncased from github\n",
    "# Bert_path = './wwm_uncased_L-24_H-1024_A-16'  # bert-large-uncased(whole word masking) from github\n",
    "\n",
    "# 初始化Bert tokenizer和模型\n",
    "tokenizer = BertTokenizer.from_pretrained(Bert_path) # bert: base or large（wwm&origin） \n",
    "\n",
    "print(tokenizer.tokenize('I have a good time, thank you.')) # 测试\n",
    "\n",
    "num_classes = len(data['category1'].unique())  # num_labels 表示分类任务中唯一类别的数量\n",
    "model = BertForSequenceClassification.from_pretrained(Bert_path, num_labels=num_classes)\n",
    "\n",
    "model_name = \"bert-base-uncased\" ## \n",
    "# num_classes\n",
    "\n",
    "\n",
    "# 定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        # label = str(self.labels[idx]) # todo: 改str试试\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long) \n",
    "        }\n",
    "    \n",
    "    \n",
    "# 创建训练和测试数据集实例\n",
    "train_dataset = CustomDataset(train_data['body'].values, train_data['label'].values, tokenizer)\n",
    "test_dataset = CustomDataset(test_data['body'].values, test_data['label'].values, tokenizer)\n",
    "\n",
    "# 使用DataLoader加载数据\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # 2.20 32-64 ## \n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # 2.20 32-64\n",
    "\n",
    "# 将模型移动到GPU上（如果可用）\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5) # 优化器可调整 学习率可调整\n",
    "optimizer = torch.optim.AdamW(model.parameters(),  lr=2e-5) # 修改新用法  优化器可调整 学习率可调整\n",
    "criterion = torch.nn.CrossEntropyLoss() # 损失函数可调整\n",
    "\n",
    "# 定义训练函数\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with tqdm(loader, desc=\"Training\", leave=False) as iterator:\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "# 定义评估函数  ## \n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    #2.20改\n",
    "    # 修改返回val_loss\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad(), tqdm(loader, desc=\"Evaluating\", leave=False) as iterator:\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "            # 修改返回val_loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * len(labels)\n",
    "            num_samples += len(labels)\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 修改返回val_loss\n",
    "    avg_loss = total_loss / num_samples\n",
    "    return all_labels, all_preds, avg_loss\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    #2.20改\n",
    "    t1,t2,val_loss = evaluate(model, test_loader, device)  # 假设evaluate函数用于计算验证集损失 ## \n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}') ## \n",
    "    # 在每个 epoch 结束时记录训练损失和验证损失\n",
    "    logging.info(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')  ## \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# 评估模型\n",
    "true_labels, predicted_labels,test_loss = evaluate(model, test_loader, device) ## \n",
    "print(true_labels[:100])\n",
    "print(predicted_labels[:100])\n",
    "# print(true_labels)\n",
    "# print(predicted_labels)\n",
    "\n",
    "# print(classification_report(true_labels, predicted_labels))\n",
    "print(classification_report(true_labels, predicted_labels,zero_division=1))\n",
    "logging.info(classification_report(true_labels, predicted_labels,zero_division=1)) ## \n",
    "\n",
    "\n",
    "model_path = f\"./models/{model_name}_classification_undersampled_new_epoch_{num_epochs}.pth\"  ## \n",
    "\n",
    "# 保存模型到文件\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# # 保存模型到文件\n",
    "# torch.save(model.state_dict(), 'bert_model_undersampled_new_e10.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7cd5e-d5da-433a-9d40-9dcf31e77f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-default",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
