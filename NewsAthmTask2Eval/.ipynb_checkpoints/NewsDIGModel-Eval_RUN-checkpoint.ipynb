{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e660f7-b749-4889-a07e-5baa6e5759d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/angle/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../NewsAthm/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 结果还行！！！！！初版完成！！！！\n",
    "# FIX：3.25 归一化后是乘以100！\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "\n",
    "# 加载训练好的模型\n",
    "model_path = '../NewsAthm/bert-base-multilingual-cased'  ## 可更换\n",
    "# modelNew_load_path = './classificationModel/bert-base-multilingual-cased_classification_undersampled_new_epoch_20.pth'  ## 可更换\n",
    "modelNew_load_path = '../NewsAthmTask2Score/classificationModel/bert-base-multilingual-cased_classification_undersampled_new_epoch_20.pth'  ## 可更换\n",
    "\n",
    "model_CLS_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=9)\n",
    "\n",
    "model.load_state_dict(torch.load(modelNew_load_path))\n",
    "model.eval()\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 定义类别列表\n",
    "categories = ['খেলাধুলা', 'রাজনীতি', 'বিনোদন', 'অর্থনীতি', 'আইন', 'শিক্ষা', 'বিজ্ঞান', 'লাইফস্টাইল', 'অন্যান্য']\n",
    "\n",
    "# 读取csv文件\n",
    "# data = pd.read_csv('./Data231202-231211/Data231202.csv')  ## \n",
    "data = pd.read_csv('./datasets/news_20240302_20240311.csv')  ## 对0302-0311这10天进行评估  从数据库爬取（定时任务）--->拿到数据--->根据date筛选\n",
    "\n",
    "data['pub_time'] = pd.to_datetime(data['pub_time'])\n",
    "\n",
    "date_UNI = '2024-03-02' ###\n",
    "# 筛选 pub_time 为 '2024-03-02' 的数据\n",
    "filtered_data = data[data['pub_time'] == date_UNI]  ## 这个日期是参数！系统端传过来后进行处理 系统传日期---》查询数据库（看是否有缓存。没有的话就现查）---》筛选\n",
    "\n",
    "# filtered_data.to_csv(\"./test0302.csv\", index=False)\n",
    "\n",
    "# 显示筛选结果\n",
    "# print(filtered_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73a569b-83b6-4ac1-a90f-50e80c51ad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "nan_check = filtered_data['body'].isna().sum()\n",
    "nan_check_c = filtered_data['category1'].isna().sum()\n",
    "print(nan_check)\n",
    "print(nan_check_c)\n",
    "\n",
    "filtered_data = filtered_data.dropna(subset=['category1','body'])\n",
    "nan_check = filtered_data['body'].isna().sum()\n",
    "nan_check_c = filtered_data['category1'].isna().sum()\n",
    "print(nan_check)\n",
    "print(nan_check_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbba4ac-6e8e-4459-8f6b-6eba8923cb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>website_id</th>\n",
       "      <th>request_url</th>\n",
       "      <th>response_url</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>pub_time</th>\n",
       "      <th>cole_time</th>\n",
       "      <th>images</th>\n",
       "      <th>language_id</th>\n",
       "      <th>md5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>20044932</td>\n",
       "      <td>2268</td>\n",
       "      <td>https://mzamin.com/news.php?news=100055</td>\n",
       "      <td>https://mzamin.com/news.php?news=100055</td>\n",
       "      <td>সংবাদ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>‘প্রত্যাগত অভিবাসী কর্মীদের প্রশিক্ষণের বিকল্প...</td>\n",
       "      <td>প্রত্যাগত অভিবাসী কর্মীদের পুন:একত্রীকরণ তথা স...</td>\n",
       "      <td>প্রত্যাগত অভিবাসী কর্মীদের পুন:একত্রীকরণ তথা স...</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>2024-03-12 08:00:34</td>\n",
       "      <td>[\"https://mzamin.com/uploads/news/main/100055_...</td>\n",
       "      <td>1779</td>\n",
       "      <td>aff97b7a708c043c23f3b769bceacfcb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>20044933</td>\n",
       "      <td>2268</td>\n",
       "      <td>https://mzamin.com/news.php?news=100034</td>\n",
       "      <td>https://mzamin.com/news.php?news=100034</td>\n",
       "      <td>সংবাদ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>নতুন রাজনৈতিক দলের ঘোষণা দিলেন শাহেদা ওবায়েদ</td>\n",
       "      <td>দেশের নতুন রাজনৈতিক দল গঠনের ঘোষণা দিয়েছেন বিএ...</td>\n",
       "      <td>দেশের নতুন রাজনৈতিক দল গঠনের ঘোষণা দিয়েছেন বিএ...</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>2024-03-12 08:00:35</td>\n",
       "      <td>[\"https://mzamin.com/uploads/news/main/100034_...</td>\n",
       "      <td>1779</td>\n",
       "      <td>32b9ec59f6855bdb48e93e9930915236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>20044934</td>\n",
       "      <td>2268</td>\n",
       "      <td>https://mzamin.com/news.php?news=100033</td>\n",
       "      <td>https://mzamin.com/news.php?news=100033</td>\n",
       "      <td>সংবাদ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>বেইলি রোডে আগুনে প্রাণহানির ঘটনায় মোদির শোক</td>\n",
       "      <td>প্রধানমন্ত্রী শেখ হাসিনাকে লেখা চিঠিতে রাজধানী...</td>\n",
       "      <td>প্রধানমন্ত্রী শেখ হাসিনাকে লেখা চিঠিতে রাজধানী...</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>2024-03-12 08:00:35</td>\n",
       "      <td>[\"https://mzamin.com/uploads/news/main/8874_mo...</td>\n",
       "      <td>1779</td>\n",
       "      <td>677a0604c17121f0ae35a44e0370e25e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>20044935</td>\n",
       "      <td>2268</td>\n",
       "      <td>https://mzamin.com/news.php?news=100032</td>\n",
       "      <td>https://mzamin.com/news.php?news=100032</td>\n",
       "      <td>সংবাদ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>সরকার জুলুম করতে পারে, জিনিসের দাম কমাতে পারে ...</td>\n",
       "      <td>সরকার জুলুম করতে পারে, জিনিসের দাম কমাতে পারে ...</td>\n",
       "      <td>সরকার জুলুম করতে পারে, জিনিসের দাম কমাতে পারে ...</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>2024-03-12 08:00:36</td>\n",
       "      <td>[\"https://mzamin.com/uploads/news/main/100032_...</td>\n",
       "      <td>1779</td>\n",
       "      <td>323e54d5efed89e92c0e14fa514a9d37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>20044936</td>\n",
       "      <td>2268</td>\n",
       "      <td>https://mzamin.com/news.php?news=100029</td>\n",
       "      <td>https://mzamin.com/news.php?news=100029</td>\n",
       "      <td>সংবাদ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ফখরুলের সঙ্গে গণতন্ত্র মঞ্চ ও ১২ দলের বৈঠক</td>\n",
       "      <td>বিএনপি মহাসচিব মির্জা ফখরুল ইসলাম আলমগীরের সঙ্...</td>\n",
       "      <td>বিএনপি মহাসচিব মির্জা ফখরুল ইসলাম আলমগীরের সঙ্...</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>2024-03-12 08:00:37</td>\n",
       "      <td>[\"https://mzamin.com/uploads/news/main/92116_B...</td>\n",
       "      <td>1779</td>\n",
       "      <td>df9d7a02ca1f3fe174e95a368f1b606d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  website_id                              request_url  \\\n",
       "2860  20044932        2268  https://mzamin.com/news.php?news=100055   \n",
       "2861  20044933        2268  https://mzamin.com/news.php?news=100034   \n",
       "2862  20044934        2268  https://mzamin.com/news.php?news=100033   \n",
       "2863  20044935        2268  https://mzamin.com/news.php?news=100032   \n",
       "2864  20044936        2268  https://mzamin.com/news.php?news=100029   \n",
       "\n",
       "                                 response_url category1 category2  \\\n",
       "2860  https://mzamin.com/news.php?news=100055     সংবাদ       NaN   \n",
       "2861  https://mzamin.com/news.php?news=100034     সংবাদ       NaN   \n",
       "2862  https://mzamin.com/news.php?news=100033     সংবাদ       NaN   \n",
       "2863  https://mzamin.com/news.php?news=100032     সংবাদ       NaN   \n",
       "2864  https://mzamin.com/news.php?news=100029     সংবাদ       NaN   \n",
       "\n",
       "                                                  title  \\\n",
       "2860  ‘প্রত্যাগত অভিবাসী কর্মীদের প্রশিক্ষণের বিকল্প...   \n",
       "2861       নতুন রাজনৈতিক দলের ঘোষণা দিলেন শাহেদা ওবায়েদ   \n",
       "2862      বেইলি রোডে আগুনে প্রাণহানির ঘটনায় মোদির শোক   \n",
       "2863  সরকার জুলুম করতে পারে, জিনিসের দাম কমাতে পারে ...   \n",
       "2864         ফখরুলের সঙ্গে গণতন্ত্র মঞ্চ ও ১২ দলের বৈঠক   \n",
       "\n",
       "                                               abstract  \\\n",
       "2860  প্রত্যাগত অভিবাসী কর্মীদের পুন:একত্রীকরণ তথা স...   \n",
       "2861  দেশের নতুন রাজনৈতিক দল গঠনের ঘোষণা দিয়েছেন বিএ...   \n",
       "2862  প্রধানমন্ত্রী শেখ হাসিনাকে লেখা চিঠিতে রাজধানী...   \n",
       "2863  সরকার জুলুম করতে পারে, জিনিসের দাম কমাতে পারে ...   \n",
       "2864  বিএনপি মহাসচিব মির্জা ফখরুল ইসলাম আলমগীরের সঙ্...   \n",
       "\n",
       "                                                   body   pub_time  \\\n",
       "2860  প্রত্যাগত অভিবাসী কর্মীদের পুন:একত্রীকরণ তথা স... 2024-03-02   \n",
       "2861  দেশের নতুন রাজনৈতিক দল গঠনের ঘোষণা দিয়েছেন বিএ... 2024-03-02   \n",
       "2862  প্রধানমন্ত্রী শেখ হাসিনাকে লেখা চিঠিতে রাজধানী... 2024-03-02   \n",
       "2863  সরকার জুলুম করতে পারে, জিনিসের দাম কমাতে পারে ... 2024-03-02   \n",
       "2864  বিএনপি মহাসচিব মির্জা ফখরুল ইসলাম আলমগীরের সঙ্... 2024-03-02   \n",
       "\n",
       "                cole_time                                             images  \\\n",
       "2860  2024-03-12 08:00:34  [\"https://mzamin.com/uploads/news/main/100055_...   \n",
       "2861  2024-03-12 08:00:35  [\"https://mzamin.com/uploads/news/main/100034_...   \n",
       "2862  2024-03-12 08:00:35  [\"https://mzamin.com/uploads/news/main/8874_mo...   \n",
       "2863  2024-03-12 08:00:36  [\"https://mzamin.com/uploads/news/main/100032_...   \n",
       "2864  2024-03-12 08:00:37  [\"https://mzamin.com/uploads/news/main/92116_B...   \n",
       "\n",
       "      language_id                               md5  \n",
       "2860         1779  aff97b7a708c043c23f3b769bceacfcb  \n",
       "2861         1779  32b9ec59f6855bdb48e93e9930915236  \n",
       "2862         1779  677a0604c17121f0ae35a44e0370e25e  \n",
       "2863         1779  323e54d5efed89e92c0e14fa514a9d37  \n",
       "2864         1779  df9d7a02ca1f3fe174e95a368f1b606d  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef5fd3b-23da-4610-9d40-47d12a221934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH!!\n"
     ]
    }
   ],
   "source": [
    "# FIX!!\n",
    "import os\n",
    "\n",
    "# 定义预测函数\n",
    "def predict_category(text):\n",
    "    # 对文本进行编码\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # 进行预测\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "    # 返回预测的类别\n",
    "    return categories[predictions.item()]\n",
    "\n",
    "# 对数据进行处理\n",
    "def process_data(data):\n",
    "    # 找出category1不在指定类别列表中的数据\n",
    "    mask = ~data['category1'].isin(categories)\n",
    "    data_to_predict = data[mask]\n",
    "\n",
    "    # 对需要预测的数据进行预测\n",
    "    data_to_predict['category1'] = data_to_predict['body'].apply(predict_category)\n",
    "\n",
    "    # 将预测后的数据与原数据合并\n",
    "    data[mask] = data_to_predict\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "processed_data_file_name = f\"./datasets/news_{date_UNI}_processed.csv\"\n",
    "# FIX:缓存操作 若已有文件则直接读取 否则才进行预测\n",
    "# 判断文件是否存在\n",
    "if os.path.exists(processed_data_file_name):\n",
    "    # 如果文件存在，则直接读取数据\n",
    "    processed_data = pd.read_csv(processed_data_file_name)\n",
    "else:\n",
    "    # 如果文件不存在，则执行处理数据的函数\n",
    "    # processed_data = process_data(data)\n",
    "    processed_data = process_data(filtered_data)\n",
    "\n",
    "    \n",
    "    # 将处理后的数据保存到文件中\n",
    "    processed_data.to_csv(processed_data_file_name, index=False)\n",
    "\n",
    "\n",
    "# # 处理数据\n",
    "# processed_data = process_data(data)\n",
    "\n",
    "# # 保存处理后的数据到新的csv文件\n",
    "# # processed_data.to_csv('./Data231202-231211/Data231202_processed.csv', index=False)\n",
    "# processed_data_file_name = f\"./datasets/news_{date_UNI}_processed.csv\"\n",
    "# processed_data.to_csv(processed_data_file_name, index=False)\n",
    "\n",
    "\n",
    "print(\"FINISH!!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91952c79-c679-4ea4-8393-999ecaf297fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# conda angle https://github.com/SeanLee97/AnglE/tree/main\n",
    "# pip install nltk\n",
    "# pip install --upgrade pip\n",
    "# pip install spacy==2.3.5\n",
    "# pip install bn_core_news_sm-0.1.0.tar.gz\n",
    "# pip install matplotlib\n",
    "import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from angle_emb import AnglE\n",
    "\n",
    "# yes! 聚类评估！！！可跑 TP, FP, TN, FN 得到RI、Precision、Recall、F1，ARI\n",
    "# update:单个成簇的处理\n",
    "from itertools import combinations\n",
    "from math import comb\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize # 使用NLTK进行分词，根据需要替换为适合孟加拉语的分词方法\n",
    "\n",
    "import spacy\n",
    "# from gensim.summarization import keywords\n",
    "from collections import defaultdict\n",
    "import bn_core_news_sm\n",
    "from sklearn.preprocessing import MinMaxScaler # 归一化\n",
    "import matplotlib.pyplot as plt\n",
    "# import pytextrank\n",
    "# =======\n",
    "# 去除停用词\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string\n",
    "# ====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75be79-c687-42f6-932f-84ce1a97ffd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18535716-78bc-40ae-b425-97ec8e44752e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[0], [1], [2, 22], [3], [4], [5], [6, 258], [7], [8, 42], [9], [10], [11], [12, 147], [13], [14, 269], [15], [16, 19], [17], [18], [20], [21, 103], [23], [24], [25], [26], [27], [28], [29], [30, 292], [31], [32, 46], [33], [34, 268], [35], [36], [37], [38], [39], [40], [41], [43], [44, 49], [45], [47, 141], [48], [50, 74], [51, 138], [52], [53], [54], [55], [56, 153], [57, 114], [58], [59, 319], [60, 382], [61], [62], [63], [64], [65], [66], [67, 233], [68], [69, 291], [70, 126], [71], [72], [73], [75], [76], [77, 315], [78], [79, 271], [80, 98], [81, 189], [82], [83, 229], [84], [85, 221], [86], [87], [88], [89], [90], [91, 190], [92], [93], [94], [95], [96], [97], [99], [100], [101], [102], [104, 209], [105], [106], [107], [108], [109], [110, 131], [111], [112], [113], [115], [116], [117], [118], [119], [120], [121, 155], [122], [123], [124], [125], [127], [128], [129, 213], [130], [132], [133], [134], [135], [136, 178], [137], [139, 234], [140], [142, 214], [143, 372], [144, 284], [145, 197], [146], [148], [149], [150], [151], [152, 211], [154], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170, 261], [171, 202], [172, 262], [173, 174], [175], [176], [177], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [191, 294], [192], [193], [194], [195], [196, 381], [198], [199], [200], [201], [203], [204], [205], [206, 227], [207, 222], [208], [210], [212, 218], [215], [216, 366], [217, 355], [219], [220], [223], [224], [225], [226, 257], [228], [230, 254], [231], [232, 238], [235], [236], [237], [239], [240, 303], [241, 246], [242], [243], [244], [245, 341], [247], [248], [249, 330], [250], [251, 376], [252], [253, 353], [255], [256, 318], [259, 277], [260], [263], [264], [265, 274], [266], [267], [270, 386], [272], [273], [275], [276], [278], [279], [280], [281], [282], [283, 359], [285, 304], [286, 344], [287, 288], [289], [290], [293], [295, 324], [296], [297, 389], [298], [299], [300], [301], [302], [305], [306], [307], [308], [309], [310], [311], [312], [313], [314], [316], [317], [320], [321], [322], [323], [325], [326], [327], [328], [329], [331], [332], [333], [334], [335], [336], [337], [338, 340], [339], [342], [343], [345], [346], [347], [348], [349], [350], [351], [352], [354], [356], [357], [358], [360], [361], [362], [363], [364], [365], [367], [368], [369], [370], [371], [373], [374], [375], [377], [378], [379], [380], [383], [384], [385], [387], [388], [390], [391], [392], [393]]\n",
      "FINISH!\n"
     ]
    }
   ],
   "source": [
    "# data_ORI = pd.read_csv('./Data231202-231211/Data231202.csv') # 所有子任务都是使用这个\n",
    "data_ORI = processed_data\n",
    "\n",
    "# 使用angle加载\n",
    "model_id = '../NewsAthmTask2/models/angle-bert-base-uncased-nli-en-v1' ## 可更换\n",
    "angle = AnglE.from_pretrained(model_id, pooling_strategy='cls_avg').cuda()\n",
    "\n",
    "# 加载数据\n",
    "data = data_ORI\n",
    "\n",
    "# 将日期转换为日期时间格式\n",
    "data['pub_time'] = pd.to_datetime(data['pub_time'])\n",
    "\n",
    "# 获取唯一日期列表\n",
    "dates = data['pub_time'].dt.date.unique()\n",
    "\n",
    "\n",
    "# 定义聚类中心更新函数\n",
    "def update_cluster_center(cluster):\n",
    "    cluster_embeddings = angle.encode(cluster, to_numpy=True) # 使用angle加载\n",
    "     \n",
    "    return np.mean(cluster_embeddings, axis=0)\n",
    "\n",
    "def get_predicted_clusters(data,threshold):\n",
    "    # 对于每个日期\n",
    "    cluster_results = []\n",
    "    cnt = 0\n",
    "    for date in dates:\n",
    "        print(cnt)\n",
    "        cnt+=1\n",
    "        # 获取该日期的新闻标题\n",
    "        news_data = data[data['pub_time'].dt.date == date]['title'].tolist()\n",
    "        # 获取该日期的新闻正文\n",
    "        # news_data = data[data['pub_time'].dt.date == date]['body'].tolist() # ByBody\n",
    "\n",
    "        embeddings = angle.encode(news_data, to_numpy=True) # 使用angle加载\n",
    "\n",
    "        # 定义当天的簇列表\n",
    "        daily_clusters = []\n",
    "\n",
    "        # 对于每个新闻数据\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            # 如果簇列表为空，则新开一个簇\n",
    "            if not daily_clusters:\n",
    "                # daily_clusters.append({'center': embedding, 'members': [news_data[i]]})\n",
    "                daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    "                continue\n",
    "\n",
    "            # 计算当前数据点与各个簇中心的相似度\n",
    "            similarities = [cosine_similarity([embedding], [cluster['center']])[0][0] for cluster in daily_clusters]\n",
    "\n",
    "            # 找到最大相似度及其对应的簇索引\n",
    "            max_similarity = max(similarities)\n",
    "            max_index = similarities.index(max_similarity)\n",
    "\n",
    "            # 如果最大相似度大于阈值，则将当前数据点加入对应簇，并更新簇中心\n",
    "            if max_similarity > threshold:\n",
    "                daily_clusters[max_index]['members'].append(i) # 改为存index\n",
    "                daily_clusters[max_index]['news'].append(news_data[i]) # 改为存index\n",
    "                daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['news'])\n",
    "            # 否则新开一个簇\n",
    "            else:\n",
    "                daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    "\n",
    "        # 将当天的簇信息添加到结果列表中\n",
    "        cluster_results.append({'date': date, 'clusters': daily_clusters})\n",
    "\n",
    "    predicted_clusters = []\n",
    "    for cluster in cluster_results[0]['clusters']: # 2023-12-02的簇s\n",
    "        clus_index = []\n",
    "        for i in cluster['members']:\n",
    "            clus_index.append(i)\n",
    "        predicted_clusters.append(clus_index)\n",
    "    print(predicted_clusters)\n",
    "    \n",
    "    return predicted_clusters\n",
    "\n",
    "# 设置阈值\n",
    "threshold = 0.972  ## 可更换\n",
    "clusters = get_predicted_clusters(data,threshold)\n",
    "\n",
    "# 创建一个字典，键是语料索引，值是对应的簇大小\n",
    "index_to_cluster_size = {index: len(cluster) for cluster in clusters for index in cluster}\n",
    "\n",
    "# 读取语料文件\n",
    "df = data_ORI\n",
    "\n",
    "# 新增列clus_news_num，记录每个语料对应的簇的大小\n",
    "df['T1_clus_news_num'] = df.index.map(index_to_cluster_size)\n",
    "\n",
    "# 根据簇大小进行排序，并添加排名，相同大小的排名相同\n",
    "df = df.sort_values(by='T1_clus_news_num', ascending=False)\n",
    "df['T1_rank'] = df['T1_clus_news_num'].rank(method='min', ascending=False)\n",
    "\n",
    "# 新增列S_scale，为簇大小的归一化结果\n",
    "scaler = MinMaxScaler()\n",
    "df['T1_S_scale'] = scaler.fit_transform(df[['T1_clus_news_num']])\n",
    "\n",
    "# # 新增列S_score，为S_scale的值乘以20\n",
    "# df['T1_S_score'] = df['T1_S_scale'] * 20\n",
    "\n",
    "# 新增列S_score，为S_scale的值乘以20\n",
    "df['T1_S_score'] = df['T1_S_scale'] * 100\n",
    "\n",
    "# 新增列index，表示语料原始的坐标\n",
    "df['T1_ori_indexFrom0'] = df.index\n",
    "\n",
    "# 只保留需要的列，并保存到新的CSV文件\n",
    "T1_final_df = df[['id','T1_ori_indexFrom0', 'title', 'body', 'T1_clus_news_num', 'T1_rank','T1_S_scale', 'T1_S_score']]\n",
    "\n",
    "# 文件保存处理，若有重名文件，则重命名为_{num}  好像并不需要 每天的是固定的 后续可能直接查询就行\n",
    "# num_file_T1 = 1\n",
    "\n",
    "# # 检查文件是否存在\n",
    "# while os.path.exists(T1_file_name):\n",
    "#     T1_file_name = f\"./T1ClusterScore/T1_{date_UNI}_result_new_{num_file_T1}.csv\"\n",
    "#     num_file_T1 += 1\n",
    "\n",
    "T1_file_name = f\"./T1ClusterScore/T1_{date_UNI}_result_new.csv\"\n",
    "T1_final_df.to_csv(T1_file_name, index=False)\n",
    "print(\"FINISH!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de71bd00-2306-4c71-b2d3-58eb1c704ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40个网站的排名以及赋分结果在./T2WebsiteRank/website_Rank_new.csv\n",
    "# Data231202-231211/Data231202.csv\n",
    "# 读取Data231202-231211/Data231202.csv，其中的website_id为网站id，现在读取./T2WebsiteRank/website_Rank_new.csv，该文件存有website_id对应的S_task_web，现在需要将Data231202.csv中的每个语料对应的website_id对应的S_task_web新增一列进行存储，然后根据S_task_web进行排序，允许并列，新增rank列，将结果中website_id,title,S_task_web,rank存到新的csv文件\n",
    "\n",
    "# 读取两个csv文件\n",
    "data_df = data_ORI\n",
    "# rank_df = pd.read_csv('./T2WebsiteRank/website_Rank_new.csv')\n",
    "rank_df = pd.read_csv('./T2WebsiteRank/website_Rank_new_FIX.csv') # FIX\n",
    "\n",
    "\n",
    "# 将两个DataFrame合并\n",
    "merged_df = pd.merge(data_df, rank_df, on='website_id')\n",
    "\n",
    "# 根据S_task_web列进行排序，并添加排名，相同权重的排名相同\n",
    "merged_df = merged_df.sort_values(by='T2_S_score', ascending=False)\n",
    "merged_df['T2_rank'] = merged_df['T2_S_score'].rank(method='min', ascending=False)\n",
    "\n",
    "# 只保留需要的列，并保存到新的CSV文件\n",
    "T2_final_df = merged_df[['id','website_id', 'title', 'T2_S_score', 'T2_rank']]\n",
    "\n",
    "T2_file_name = f\"./T2WebsiteRank/T2_{date_UNI}_result_new.csv\" ## FIX\n",
    "# T2_final_df.to_csv('./T2WebsiteRank/Data231202_scoreResult.csv', index=False)\n",
    "T2_final_df.to_csv(T2_file_name, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01c77133-66c4-418c-b163-4a98f93c7512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，并将结果保存到新的CSV文件中。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 读取CSV文件并计算正文长度\n",
    "df = data_ORI\n",
    "df['body_len'] = df['body'].apply(lambda x: len(str(x).split()))  # 假设每个单词之间用空格分隔\n",
    "\n",
    "# 按正文长度进行排序\n",
    "df = df.sort_values(by='body_len', ascending=False)\n",
    "\n",
    "# 添加排名列\n",
    "df['T3_rank'] = df['body_len'].rank(method='min', ascending=False)\n",
    "\n",
    "# 计算S_scale并添加列\n",
    "max_len = df['body_len'].max()\n",
    "min_len = df['body_len'].min()\n",
    "df['T3_S_scale'] = (df['body_len'] - min_len) / (max_len - min_len)\n",
    "\n",
    "# # 计算body_len_score并添加列\n",
    "# df['T3_S_score'] = 20 * df['T3_S_scale']\n",
    "\n",
    "# 计算body_len_score并添加列\n",
    "df['T3_S_score'] = 100 * df['T3_S_scale'] #FIX\n",
    "\n",
    "# 保存结果到新的CSV文件\n",
    "T3_file_name_1 = f\"./T3BodyLenRank/T3_{date_UNI}_result_new_all.csv\"\n",
    "T3_file_name_2 = f\"./T3BodyLenRank/T3_{date_UNI}_result_new.csv\"\n",
    "\n",
    "# output_file = './T3BodyLenRank/Data231202_newDATA_rank_Score_new.csv'  # 替换为你的输出文件路径\n",
    "# df.to_csv(output_file, index=False)\n",
    "df.to_csv(T3_file_name_1, index=False)\n",
    "\n",
    "\n",
    "# 只保留需要的列，并保存到新的CSV文件\n",
    "T3_final_df = df[['id','title', 'body_len', 'T3_rank','T3_S_scale', 'T3_S_score']]\n",
    "# T3_final_df.to_csv('./T3BodyLenRank/Data231202_T3scoreResult.csv', index=False)\n",
    "T3_final_df.to_csv(T3_file_name_2, index=False)\n",
    "\n",
    "print(\"处理完成，并将结果保存到新的CSV文件中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6698c061-4cca-438a-827c-7c8a869f8dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~‘’\n",
      "<class 'list'>\n",
      "filtered_titles len\n",
      " 2423\n",
      "17422\n",
      "2423\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 加载孟加拉语模型\n",
    "nlp = bn_core_news_sm.load()\n",
    "# # textrank算法计算权重\n",
    "# update 3.9：改进版！！\n",
    "def textrank_weighted_word_graph(merged_titles):\n",
    "    tokens = nlp(merged_titles) # 分词\n",
    "    print(len(tokens))\n",
    "    # print(tokens)\n",
    "    \n",
    "    graph = nx.Graph()\n",
    "    window_size = 80  # 根据需要调整窗口大小\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        for j in range(i+1, min(i+window_size+1, len(tokens))):\n",
    "            if token != tokens[j]:  # 添加边,避免自环\n",
    "                if graph.has_edge(token, tokens[j]):\n",
    "                    graph[token][tokens[j]]['weight'] += 1 #在添加边时,先检查边是否已经存在。如果边已经存在,则将权重加1;否则,添加一个新边,权重为1。这样可以避免重复添加边。\n",
    "                else:\n",
    "                    graph.add_edge(token, tokens[j], weight=1)\n",
    "    \n",
    "    # 使用NetworkX的PageRank算法计算每个节点（词）的权重\n",
    "    pagerank_scores = nx.pagerank(graph, weight='weight')\n",
    "\n",
    "    return pagerank_scores,graph\n",
    "\n",
    "# 读取CSV文件并合并所有标题\n",
    "df = data_ORI\n",
    "\n",
    "merged_titles = ' '.join(title.strip() for title in df['title'])\n",
    "\n",
    "# ====================================\n",
    "# 获取孟加拉语的停用词列表\n",
    "stop_words = set(stopwords.words('bengali'))\n",
    "# print(stop_words)\n",
    "\n",
    "# 自定义标点符号列表\n",
    "custom_punctuation = ['‘', '’']\n",
    "\n",
    "# 合并 NLTK 提供的标点符号列表和自定义标点符号列表\n",
    "all_punctuation = string.punctuation + ''.join(custom_punctuation)\n",
    "\n",
    "print(all_punctuation)\n",
    "# 分词# word_tokens = word_tokenize(merged_titles)\n",
    "\n",
    "word_tokens = nlp(merged_titles) # 分词\n",
    "# word_tokens = merged_titles.split() # 根据空格分词\n",
    "token_texts = [token.text.strip() for token in word_tokens] # 去除多余空格\n",
    "\n",
    "# print(token_texts)\n",
    "print(type(token_texts))\n",
    "\n",
    "\n",
    "\n",
    "# 去除停用词\n",
    "# filtered_titles = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_titles = [w for w in token_texts if not w in stop_words] # 去除停用词\n",
    "filtered_titles = [word for word in filtered_titles if word not in all_punctuation] # 去除标点符号\n",
    "\n",
    "print(\"filtered_titles len\\n\",len(filtered_titles)) # 字符串数量！\n",
    "\n",
    "# 将去除停用词后的词重新组合成字符串\n",
    "filtered_titles_text = ' '.join(filtered_titles)\n",
    "\n",
    "print(len(filtered_titles_text)) # 字符串长度！别被误导（所少个字符）\n",
    "# ====================================\n",
    "\n",
    "# 计算词权重\n",
    "word_weights,graph = textrank_weighted_word_graph(filtered_titles_text)\n",
    "\n",
    "# 保存pagerank算法后的词关系权重 可视化\n",
    "# 根据PageRank值更新边的权重\n",
    "# 记录权重关系 字典形式存储\n",
    "pagerank_weighted_graph = nx.Graph()\n",
    "for node, score in word_weights.items():\n",
    "    pagerank_weighted_graph.add_node(node)\n",
    "\n",
    "for u, v, data in graph.edges(data=True):\n",
    "    weight = data['weight'] * word_weights[u] * word_weights[v]\n",
    "    pagerank_weighted_graph.add_edge(u, v, weight=weight)\n",
    "\n",
    "graph_content_file_name = f\"./T4TitleTextRank/T4_{date_UNI}_graph_content.txt\"\n",
    "with open('./T4TitleTextRank/graph_content.txt', 'w') as file:\n",
    "    file.write(str(nx.to_dict_of_dicts(pagerank_weighted_graph)))\n",
    "\n",
    "sorted_words = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 保存到新的CSV文件\n",
    "# word_weights_df = pd.DataFrame(word_weights.items(), columns=['word', 'weight'])\n",
    "word_weights_df = pd.DataFrame(sorted_words, columns=['word', 'weight'])\n",
    "\n",
    "word_weight_file_name = f\"./T4TitleTextRank/T4_{date_UNI}_word_weight_new.csv\"\n",
    "\n",
    "# word_weights_df.to_csv('./T4TitleTextRank/word_weight.csv', index=False)\n",
    "# word_weights_df.to_csv('./T4TitleTextRank/word_weight_new.csv', index=False)\n",
    "word_weights_df.to_csv(word_weight_file_name, index=False)\n",
    "\n",
    "# 接下来，计算每个标题的权重\n",
    "# 读取词权重文件\n",
    "# word_weights_df = pd.read_csv('./T4TitleTextRank/word_weight.csv')\n",
    "# word_weights_df = pd.read_csv('./T4TitleTextRank/word_weight_new.csv')\n",
    "word_weights_df = pd.read_csv(word_weight_file_name)\n",
    "\n",
    "\n",
    "# 将词权重转换为字典，方便查找\n",
    "word_weights = pd.Series(word_weights_df.weight.values, index=word_weights_df.word).to_dict()\n",
    "\n",
    "# print(word_weights)\n",
    "# 读取新闻标题文件\n",
    "titles_df = data_ORI\n",
    "# titles_df = pd.read_csv('./Data231202-231211/Data231202.csv')\n",
    "# titles_df = titles_df['title']\n",
    "\n",
    "\n",
    "\n",
    "# 定义一个函数，用于计算标题的权重\n",
    "def calculate_title_weight(title):\n",
    "    doc = nlp(title)\n",
    "    # 对标题进行分词并计算总权重\n",
    "    return sum(word_weights.get(token.text, 0) for token in doc)  # 如果词不在word_weights中，则默认权重为0\n",
    "    # return sum(word_weights.get(token.text, 0) for token in doc if token.text not in stop_words and token.text not in all_punctuation)  # 如果词不在word_weights中，则默认权重为0\n",
    "    # return sum(word_weights.get(token.text, 0) for token in doc if token.text not in stop_words and token.text not in string.punctuation)  # 如果词不在word_weights中，则默认权重为0\n",
    "\n",
    "\n",
    "# 计算每个标题的权重\n",
    "titles_df['T4_title_weight'] = titles_df['title'].apply(calculate_title_weight)\n",
    "# print(titles_df['T4_title_weight'])\n",
    "\n",
    "# 根据权重排序并添加排名，相同权重的排名相同\n",
    "titles_df = titles_df.sort_values(by='T4_title_weight', ascending=False)\n",
    "titles_df['T4_rank'] = titles_df['T4_title_weight'].rank(method='min', ascending=False)\n",
    "\n",
    "# 对权重进行归一化处理，并存储结果到\"S_scale\"列\n",
    "scaler = MinMaxScaler()\n",
    "titles_df['T4_S_scale'] = scaler.fit_transform(titles_df[['T4_title_weight']])  # 归一化映射到分数！\n",
    "\n",
    "# # 创建\"S_score\"列\n",
    "# titles_df['T4_S_score'] = titles_df['T4_S_scale'] * 20\n",
    "\n",
    "# 创建\"S_score\"列\n",
    "titles_df['T4_S_score'] = titles_df['T4_S_scale'] * 100\n",
    "\n",
    "# 只保留需要的列\n",
    "T4_final_df = titles_df[['id','title', 'T4_title_weight', 'T4_rank', 'T4_S_scale', 'T4_S_score']]\n",
    "\n",
    "\n",
    "# 保存到新的csv文件\n",
    "# final_df.to_csv('./T4TitleTextRank/titles_weight.csv', index=False)\n",
    "\n",
    "T4_file_name = f\"./T4TitleTextRank/T4_{date_UNI}_result_new.csv\"\n",
    "# T4_final_df.to_csv('./T4TitleTextRank/titles_weight_new.csv', index=False)\n",
    "T4_final_df.to_csv(T4_file_name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a042abc9-01b0-4f8a-ac4f-a49ee2f6a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 提取新闻的category1进行类别评分\n",
    "\n",
    "# category_df = pd.read_csv('./T5CateforyScore/category_score.csv')\n",
    "category_df = pd.read_csv('./T5CateforyScore/category_score_FIX.csv')\n",
    "\n",
    "\n",
    "# Load the CSV file with news data\n",
    "# news_df = pd.read_csv('./Data231202-231211_FIX/Data231202_newDATA.csv')\n",
    "news_df = data_ORI\n",
    "\n",
    "\n",
    "# Merge the two DataFrames based on the \"category1\" column\n",
    "merged_df = pd.merge(news_df, category_df, how='left', left_on='category1', right_on='category')\n",
    "\n",
    "# Sort the merged DataFrame based on the \"rank\" column\n",
    "sorted_df = merged_df.sort_values(by='T5_rank')\n",
    "\n",
    "# Select the desired columns\n",
    "selected_columns = ['id','title', 'category1', 'T5_rank', 'T5_S_scale', 'T5_S_score']\n",
    "T5_final_df = sorted_df[selected_columns]\n",
    "\n",
    "T5_file_name = f\"./T5CateforyScore/T5_{date_UNI}_result_new.csv\"\n",
    "# Save the result to a new CSV file\n",
    "# T5_final_df.to_csv('./T5CateforyScore/Data231202_categoryScore_new.csv', index=False)\n",
    "T5_final_df.to_csv(T5_file_name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f34e23de-6f51-4a51-a86c-1912cf58520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# T1_final_df :'id','T1_ori_indexFrom0', 'title', 'body', 'T1_clus_news_num', 'T1_rank','T1_S_scale', 'T1_S_score'\n",
    "# T2_final_df:'id','website_id', 'title', 'T2_S_score', 'T2_rank'\n",
    "# T3_final_df:'id','title', 'body_len', 'T3_rank','T3_S_scale', 'T3_S_score'\n",
    "# T4_final_df: 'id','title', 'T4_title_weight', 'T4_rank', 'T4_S_scale', 'T4_S_score'\n",
    "# T5_final_df:'id','title', 'category1', 'T5_rank', 'T5_S_scale', 'T5_S_score'\n",
    "# 合并5个dataframe：\n",
    "# 第一步:将T1_final_df和T2_final_df合并\n",
    "merged_df = pd.merge(T1_final_df, T2_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "# 第二步:将第一步合并后的DataFrame与T3_final_df合并\n",
    "merged_df = pd.merge(merged_df, T3_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "# 第三步:将第二步合并后的DataFrame与T4_final_df合并\n",
    "merged_df = pd.merge(merged_df, T4_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "# 第四步:将第三步合并后的DataFrame与T5_final_df合并\n",
    "merged_df = pd.merge(merged_df, T5_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "# 打印合并后的 DataFrame\n",
    "Merge_file_name = f\"./MergeFiveDScore/Merge_{date_UNI}_FiveDScore_result_new.csv\"\n",
    "# merged_df.to_csv('./MergeFiveDScore/FiveDScore_Merge.csv', index=False)\n",
    "merged_df.to_csv(Merge_file_name, index=False)\n",
    "\n",
    "# print(merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1849acb-8245-462f-bc4c-b3e1fde35554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 假设权重 \n",
    "# w1, w2, w3, w4, w5 = 0.5,0.05,0.05,0.3,0.1\n",
    "# 权重设置思路：\n",
    "# ①层次分析法 根据各任务的重要性赋权\n",
    "# ②迭代 需要一个评估指标（正确个数？）来进行迭代找出模型最优权重！\n",
    "\n",
    "# 层次分析法权重！：\n",
    "# 通过进行层次分析法确定的五个维度权重为:相似新闻报道频率(0.46221)、新闻来源网站权威性(0.03503)、新闻标题重要性(0.35029)、新闻正文长度(0.03049)、新闻类别(0.12198)。\n",
    "# 对应\n",
    "# T1：0.46221 相似新闻---clusterScore\n",
    "# T2: 0.03503 网站权威性---WebsiteRank \n",
    "# T3：0.03049 正文长度---bodyLenRank\n",
    "# T4：0.35029 新闻标题重要性 --- TitleTextRank\n",
    "# T5：0.12198 新闻类别 --- Category\n",
    "\n",
    "w1, w2, w3, w4, w5 = 0.46221, 0.03503, 0.03049, 0.35029, 0.12198\n",
    "\n",
    "\n",
    "# 计算总分数\n",
    "merged_df['total_S_score'] = w1 * merged_df['T1_S_score'] + w2 * merged_df['T2_S_score'] + w3 * merged_df['T3_S_score'] + w4 * merged_df['T4_S_score'] + w5 * merged_df['T5_S_score']\n",
    "\n",
    "# 生成排名\n",
    "merged_df['total_rank'] = merged_df['total_S_score'].rank(method='min', ascending=False)\n",
    "\n",
    "# 根据总分数降序排序\n",
    "merged_df = merged_df.sort_values('total_S_score', ascending=False)\n",
    "\n",
    "# 将结果保存到csv文件\n",
    "total_result_file_name = f\"./MergeFiveDScore/total_result_{date_UNI}.csv\"\n",
    "# merged_df.to_csv('./MergeFiveDScore/total_result.csv', index=False)\n",
    "merged_df.to_csv(total_result_file_name , index=False)\n",
    "\n",
    "\n",
    "selected_columns = ['id','T1_ori_indexFrom0', 'category1','title','body','total_S_score','total_rank']\n",
    "merged_df_pure =  merged_df[selected_columns]\n",
    "\n",
    "total_result_pure_file_name = f\"./MergeFiveDScore/total_result_pure_{date_UNI}.csv\"\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "# merged_df_pure.to_csv('./MergeFiveDScore/total_result_pure.csv', index=False)\n",
    "merged_df_pure.to_csv(total_result_pure_file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec0958-9052-427c-b0cd-f643db66c204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-angle",
   "language": "python",
   "name": "conda-angle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
