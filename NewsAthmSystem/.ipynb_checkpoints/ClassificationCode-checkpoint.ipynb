{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5512a1-8fc6-421c-847a-439e5c085f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "原始数据 --- 数据去重 --- 类别筛选 --- 类别合并 --- 数据平衡 --- 分类模型 --- 分类后文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98043dd-d58e-477a-8b14-9331393d991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去重：\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43cabcb-8dde-4917-8871-3708bee23d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语料去重 （可运行）\n",
    "\n",
    "# 读取CSV文件\n",
    "# file_path = 'mangoNews_Example.csv'  # 测试版数据集（规模小5m）\n",
    "# file_path = 'datasets/mangoNews.csv'          # 完整版数据集（13g） 需要分块读\n",
    "# file_path = 'datasets/mangoNews_Example_100000.csv'          # 100000行数据集（2.2g）\n",
    "file_path = 'mangoNews_Example_10000.csv'          # 10000行数据集（190m)\n",
    "\n",
    "data = pd.read_csv(file_path,low_memory=False,lineterminator=\"\\n\")\n",
    "\n",
    "# 统计去重前的总语料数\n",
    "total_records_before = len(data)\n",
    "\n",
    "# 根据多个列进行去重，并保留重复行\n",
    "# duplicates = data[data.duplicated(subset=[\"website_id\", \"request_url\",\"response_url\",\"category1\", \"category2\", \"title\", \"body\",\"pub_time\"], keep=False)]\n",
    "\n",
    "# 输出重复行\n",
    "# print(\"Duplicate records:\")\n",
    "# print(duplicates)\n",
    "\n",
    "# 对 'body'列内容都重复的语料进行去重处理\n",
    "# data.drop_duplicates(subset=['body'], inplace=True)\n",
    "# data.drop_duplicates(subset=[\"category1\", \"category2\", \"title\", \"body\"], inplace=True) # 100025  99352\n",
    "# data.drop_duplicates(subset=[\"website_id\", \"request_url\", \"category1\", \"category2\", \"title\", \"body\"], inplace=True) # 100025  99534\n",
    "# data.drop_duplicates(subset=[\"website_id\", \"request_url\",\"response_url\",\"category1\", \"category2\", \"title\", \"body\"], inplace=True) # 100025  99534\n",
    "# data.drop_duplicates(subset=[\"website_id\", \"request_url\",\"response_url\",\"category1\", \"category2\", \"title\", \"body\",\"pub_time\"], inplace=True) # 100025  99829\n",
    "# data.drop_duplicates(subset=[\"website_id\", \"request_url\",\"response_url\",\"category1\", \"category2\", \"title\", \"body\",\"pub_time\",\"cole_time\"], inplace=True) # 100025  100019\n",
    "# data.drop_duplicates(subset=[\"website_id\", \"request_url\",\"response_url\",\"category1\", \"category2\", \"title\", \"body\",\"pub_time\",\"cole_time\",\"images\"], inplace=True) # 100025  100022\n",
    "# 不同时间爬到同一篇（除爬取时间和id外其他相同）\n",
    "data.drop_duplicates(subset=[\"website_id\", \"request_url\",\"response_url\",\"category1\", \"category2\", \"title\", \"abstract\",\"body\",\"pub_time\",\"images\"], inplace=True) # 100025  99862\n",
    "\n",
    "# data.drop_duplicates(subset=[\"md5\"], inplace=True) # 100025  99474\n",
    "\n",
    "\n",
    "# 统计去重后的总语料数\n",
    "total_records_after = len(data)\n",
    "\n",
    "data.to_csv('datasets_FIX/FIX_deduplicated_mangoNews.csv', index=False)\n",
    "\n",
    "\n",
    "# 打印统计结果\n",
    "print(f\"Total records before deduplication: {total_records_before}\")\n",
    "print(f\"Total records after deduplication: {total_records_after}\")\n",
    "# 10000：10007 9969\n",
    "# 100000:100025 98912\n",
    "\n",
    "# new tragedy： 1663846 1661972 !!! new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10eee40-17bb-4c5d-9a6d-3b1d6049d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "\n",
    "def DateDataProcess_Impl(date_sel):\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from transformers import BertTokenizer, BertForSequenceClassification\n",
    "    from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "    import torch.nn as nn\n",
    "    import os\n",
    "    # 定义Self-Attention层\n",
    "    class SelfAttention(nn.Module):\n",
    "        def __init__(self, hidden_size):\n",
    "            super(SelfAttention, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.projection = nn.Sequential(\n",
    "                nn.Linear(hidden_size, 64),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        def forward(self, encoder_outputs):\n",
    "            energy = self.projection(encoder_outputs)\n",
    "            weights = torch.softmax(energy.squeeze(-1), dim=1)\n",
    "            outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
    "            return outputs\n",
    "\n",
    "\n",
    "        # 定义模型\n",
    "    class NewsClassifier(nn.Module):\n",
    "        # hidden_size = 128\n",
    "        def __init__(self, bert_model,num_classes, hidden_size, num_layers=2, bidirectional=True):\n",
    "            super(NewsClassifier, self).__init__()\n",
    "            self.bert = bert_model # FIX\n",
    "\n",
    "            self.lstm = nn.LSTM(input_size=bert_model.config.hidden_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "                                bidirectional=bidirectional, batch_first=True) # FIX\n",
    "\n",
    "            self.attention = SelfAttention(hidden_size * (2 if bidirectional else 1))\n",
    "            self.fc = nn.Linear(hidden_size * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            lstm_outputs, _ = self.lstm(last_hidden_state)\n",
    "            attention_outputs = self.attention(lstm_outputs)\n",
    "            logits = self.fc(attention_outputs)\n",
    "            return logits\n",
    "\n",
    "\n",
    "\n",
    "    # 加载训练好的模型\n",
    "    model_path = './models/bert-base-multilingual-cased'  ## 可更换\n",
    "    # modelNew_load_path = './classificationModel/bert-base-multilingual-cased_classification_undersampled_new_epoch_20.pth'  ## 可更换\n",
    "    # modelNew_load_path = '../NewsAthmTask2Score/classificationModel/bert-base-multilingual-cased_classification_undersampled_new_epoch_20.pth'  ## 可更换\n",
    "    modelNew_load_path = './classificationModel/best_MultiBert_BiLSTM_SelfAttention_modelFIX_fold_5.pth'  ## 可更换\n",
    "\n",
    "    model_CLS_name = \"mbert_BiLSTM_SelfAttention\" ###!!!\n",
    "\n",
    "    # model = BertForSequenceClassification.from_pretrained(model_path, num_labels=9)\n",
    "    model = BertModel.from_pretrained(model_path)\n",
    "\n",
    "    # 加载tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # 将模型移动到GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # print(device)\n",
    "    # model.to(device)\n",
    "\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # 设置超参数\n",
    "    max_length = 512\n",
    "    hidden_size = 128\n",
    "    num_classes = 9\n",
    "    num_layers = 2\n",
    "    bidirectional = True\n",
    "\n",
    "\n",
    "    # 加载训练好的模型\n",
    "    # modelNew_load_path = '../NewsAthmTask2Score/classificationModel/best_MultiBert_BiLSTM_SelfAttention_modelFIX_fold_5.pth'  ## 可更换\n",
    "    model = NewsClassifier(bert_model = model,num_classes=num_classes, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "    model.load_state_dict(torch.load(modelNew_load_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 定义类别列表\n",
    "    # categories = ['খেলাধুলা', 'রাজনীতি', 'বিনোদন', 'অর্থনীতি', 'আইন', 'শিক্ষা', 'বিজ্ঞান', 'লাইফস্টাইল', 'অন্যান্য']\n",
    "    # categories = ['রাজনীত','লাইফস্টাইল','শিক্ষা','অর্থনীতি','খেলাধুলা','অন্যান্য','বিজ্ঞান','বিনোদন', 'আইন'] # FIX!!!!!!!!!!!!\n",
    "    \n",
    "    categories = ['রাজনীতি','লাইফস্টাইল','শিক্ষা','অর্থনীতি','খেলাধুলা','অন্যান্য','বিজ্ঞান','বিনোদন', 'আইন'] # FIX!!!!!!!!!!!!   政治改正\n",
    "\n",
    "    # [‘政治’、‘生活方式’、‘教育’、‘经济’、‘体育’、‘其他’、‘科学’、‘娱乐’、‘法律’]\n",
    "\n",
    "    # 定义数据处理函数\n",
    "    def preprocess_data(text, tokenizer, max_length):\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return encoding['input_ids'].to(device), encoding['attention_mask'].to(device)\n",
    "\n",
    "    # 定义预测函数\n",
    "    def predict(model, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "        return preds.item(), categories[preds.item()]\n",
    "\n",
    "\n",
    "    data_file_path = f'./datasets/mbert_BiLSTM_SelfAttention/{date_sel}/news_{date_sel}.csv'\n",
    "    # data = pd.read_csv('./datasets/news_20240302_20240318.csv')  ## 对0302-0318这进行评估  从数据库爬取（定时任务）--->拿到数据--->根据date筛选\n",
    "    data = pd.read_csv(data_file_path)  ## 对0302-0318这进行评估  从数据库爬取（定时任务）--->拿到数据--->根据date筛选\n",
    "\n",
    "    data['pub_time'] = pd.to_datetime(data['pub_time'])\n",
    "\n",
    "    # date_UNI = '2024-03-12' ###\n",
    "    date_UNI = date_sel ###\n",
    "\n",
    "    # 筛选 pub_time 为 '2024-03-02' 的数据\n",
    "    filtered_data = data[data['pub_time'] == date_UNI]  ## 这个日期是参数！系统端传过来后进行处理 系统传日期---》查询数据库（看是否有缓存。没有的话就现查）---》筛选\n",
    "\n",
    "    # filtered_data.to_csv(\"./test0302.csv\", index=False)\n",
    "\n",
    "    # 显示筛选结果\n",
    "    # print(filtered_data)\n",
    "\n",
    "\n",
    "    nan_check = filtered_data['body'].isna().sum()\n",
    "    nan_check_c = filtered_data['category1'].isna().sum()\n",
    "    print(nan_check)\n",
    "    print(nan_check_c)\n",
    "\n",
    "    filtered_data = filtered_data.dropna(subset=['category1','body'])\n",
    "    nan_check = filtered_data['body'].isna().sum()\n",
    "    nan_check_c = filtered_data['category1'].isna().sum()\n",
    "    print(nan_check)\n",
    "    print(nan_check_c)\n",
    "\n",
    "\n",
    "    processed_data_file_name = f\"./datasets/{model_CLS_name}/news_{date_UNI}_processed_{model_CLS_name}.csv\"\n",
    "    \n",
    "    # FIX:缓存操作 若已有文件则直接读取 否则才进行预测\n",
    "    # 判断文件是否存在\n",
    "    if os.path.exists(processed_data_file_name):\n",
    "        # 如果文件存在，则直接读取数据\n",
    "        processed_data = pd.read_csv(processed_data_file_name)\n",
    "    else:\n",
    "        # 如果文件不存在，则执行处理数据的函数\n",
    "        # processed_data = process_data(data)\n",
    "        # processed_data = process_data(filtered_data)\n",
    "\n",
    "        # FIX:\n",
    "        predicted_categories = []\n",
    "        cnt = 0\n",
    "        for idx, row in filtered_data.iterrows():\n",
    "            cnt+=1\n",
    "            if cnt%200 == 0: \n",
    "                print(\"categoryProcessing\")\n",
    "                print(cnt)\n",
    "            if row['category1'] not in categories:\n",
    "                input_ids, attention_mask = preprocess_data(row['body'], tokenizer, max_length)\n",
    "                pred_id, predicted_category = predict(model, input_ids, attention_mask)\n",
    "                predicted_categories.append(predicted_category)\n",
    "            else:\n",
    "                predicted_categories.append(row['category1'])\n",
    "\n",
    "        # 将预测后的类别替换原有的category1列\n",
    "        filtered_data['category1'] = predicted_categories\n",
    "        processed_data = filtered_data\n",
    "\n",
    "        # 将处理后的数据保存到文件中\n",
    "        processed_data.to_csv(processed_data_file_name, index=False)\n",
    "\n",
    "    print(\"FINISH!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-default",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
